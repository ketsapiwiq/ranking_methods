{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "cache_dir = input(\"Indicate path to all Hugging Face caches:\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = cache_dir\n",
    "os.environ[\"HF_HUB_CACHE\"] = cache_dir\n",
    "os.environ[\"HF_TOKEN\"] = getpass(\"Enter your HuggingFace token:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since ministere-culture/comparia-reactions couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'default' at /home/jupyterhub-users/shared/projet_comparia/huggingface_hub/ministere-culture___comparia-reactions/default/0.0.0/92a324c10228176065909b52bbbaa16430e64c5a (last modified on Wed Jun  4 17:40:33 2025).\n",
      "Using the latest cached version of the dataset since ministere-culture/comparia-conversations couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'default' at /home/jupyterhub-users/shared/projet_comparia/huggingface_hub/ministere-culture___comparia-conversations/default/0.0.0/dc40af6af1c14e68bf39d55f6e1573d2d6582f19 (last modified on Wed Jun  4 17:40:30 2025).\n"
     ]
    }
   ],
   "source": [
    "from rank_comparia.utils import load_comparia\n",
    "\n",
    "reactions = load_comparia(\"ministere-culture/comparia-reactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul des scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule des scores comme dans le notebook `rankers.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_comparia.data_transformation import get_matches_with_score\n",
    "\n",
    "matches = get_matches_with_score(reactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_a_name</th><th>model_b_name</th><th>conversation_pair_id</th><th>score_a</th><th>score_b</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;llama-3.1-405b&quot;</td><td>&quot;deepseek-v3-chat&quot;</td><td>&quot;fa310c827c8742f8aac9c3a0b80684…</td><td>0</td><td>4</td></tr><tr><td>&quot;llama-3.1-8b&quot;</td><td>&quot;mixtral-8x22b-instruct-v0.1&quot;</td><td>&quot;74265ff06f26405c9854ba82cb017e…</td><td>2</td><td>-1</td></tr><tr><td>&quot;claude-3-5-sonnet-v2&quot;</td><td>&quot;llama-3.1-70b&quot;</td><td>&quot;95dc4d4aeb9c4ab29227142ea6c8b3…</td><td>-1</td><td>0</td></tr><tr><td>&quot;gemini-2.0-flash-exp&quot;</td><td>&quot;qwen2.5-coder-32b-instruct&quot;</td><td>&quot;278e1b61621c47a68cc41b14ef1621…</td><td>2</td><td>-2</td></tr><tr><td>&quot;llama-3.3-70b&quot;</td><td>&quot;gemini-2.0-flash-001&quot;</td><td>&quot;a2de15a249b54200a4c6aa8f14fd48…</td><td>-2</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌──────────────────────┬───────────────────────────┬───────────────────────────┬─────────┬─────────┐\n",
       "│ model_a_name         ┆ model_b_name              ┆ conversation_pair_id      ┆ score_a ┆ score_b │\n",
       "│ ---                  ┆ ---                       ┆ ---                       ┆ ---     ┆ ---     │\n",
       "│ str                  ┆ str                       ┆ str                       ┆ i64     ┆ i64     │\n",
       "╞══════════════════════╪═══════════════════════════╪═══════════════════════════╪═════════╪═════════╡\n",
       "│ llama-3.1-405b       ┆ deepseek-v3-chat          ┆ fa310c827c8742f8aac9c3a0b ┆ 0       ┆ 4       │\n",
       "│                      ┆                           ┆ 80684…                    ┆         ┆         │\n",
       "│ llama-3.1-8b         ┆ mixtral-8x22b-instruct-v0 ┆ 74265ff06f26405c9854ba82c ┆ 2       ┆ -1      │\n",
       "│                      ┆ .1                        ┆ b017e…                    ┆         ┆         │\n",
       "│ claude-3-5-sonnet-v2 ┆ llama-3.1-70b             ┆ 95dc4d4aeb9c4ab29227142ea ┆ -1      ┆ 0       │\n",
       "│                      ┆                           ┆ 6c8b3…                    ┆         ┆         │\n",
       "│ gemini-2.0-flash-exp ┆ qwen2.5-coder-32b-instruc ┆ 278e1b61621c47a68cc41b14e ┆ 2       ┆ -2      │\n",
       "│                      ┆ t                         ┆ f1621…                    ┆         ┆         │\n",
       "│ llama-3.3-70b        ┆ gemini-2.0-flash-001      ┆ a2de15a249b54200a4c6aa8f1 ┆ -2      ┆ 1       │\n",
       "│                      ┆                           ┆ 4fd48…                    ┆         ┆         │\n",
       "└──────────────────────┴───────────────────────────┴───────────────────────────┴─────────┴─────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_comparia.elo import ELORanker\n",
    "from rank_comparia.ranker import Match, MatchScore\n",
    "import random\n",
    "\n",
    "\n",
    "def compute_match_score(score_a: int, score_b: int) -> MatchScore:\n",
    "    final_score = score_b - score_a\n",
    "    if final_score > 0:\n",
    "        return MatchScore.B\n",
    "    elif final_score < 0:\n",
    "        return MatchScore.A\n",
    "    else:\n",
    "        return MatchScore.Draw\n",
    "\n",
    "\n",
    "def get_shuffled_results(matches: list[Match], model_names: list[str], seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    ranker_shuffle = ELORanker(K=40)\n",
    "    matches_shuffle = random.sample(matches, k=len(matches))\n",
    "    ranker_shuffle.add_players(model_names)\n",
    "    ranker_shuffle.compute_scores(matches=matches_shuffle)\n",
    "    return ranker_shuffle.players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = set(matches[\"model_a_name\"].unique()) | set(matches[\"model_b_name\"].unique())\n",
    "matches = [\n",
    "    Match(\n",
    "        match_dict[\"model_a_name\"],\n",
    "        match_dict[\"model_b_name\"],\n",
    "        compute_match_score(match_dict[\"score_a\"], match_dict[\"score_b\"]),\n",
    "    )\n",
    "    for match_dict in matches.to_dicts()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gemma-3-27b': 1191.3395006565374,\n",
       " 'gemini-2.0-flash-exp': 1144.6204338742177,\n",
       " 'gpt-4.1-mini': 1136.9810148078843,\n",
       " 'claude-3-7-sonnet': 1136.9727109548724,\n",
       " 'llama-3.1-nemotron-70b-instruct': 1129.535115932537,\n",
       " 'deepseek-v3-0324': 1127.343019633871,\n",
       " 'command-a': 1124.262695420955,\n",
       " 'gemma-3-12b': 1110.973006187927,\n",
       " 'gemini-2.0-flash-001': 1096.177320994962,\n",
       " 'grok-3-mini-beta': 1095.5316626903282,\n",
       " 'gemini-1.5-pro-002': 1094.2971057766233,\n",
       " 'llama-4-scout': 1086.8984357930387,\n",
       " 'mistral-large-2411': 1079.1626953808234,\n",
       " 'gemma-3-4b': 1067.4333848739445,\n",
       " 'claude-3-5-sonnet-v2': 1061.5224933944878,\n",
       " 'deepseek-v3-chat': 1055.8583879004634,\n",
       " 'gpt-4.1-nano': 1043.3380939198712,\n",
       " 'gemini-1.5-pro-001': 1041.22455039952,\n",
       " 'qwq-32b': 1024.5967895935885,\n",
       " 'mistral-small-3.1-24b': 1024.3801866853216,\n",
       " 'gpt-4o-2024-08-06': 1019.9560175155277,\n",
       " 'deepseek-r1': 1008.2185359046271,\n",
       " 'mixtral-8x7b-instruct-v0.1': 1000.331257052885,\n",
       " 'c4ai-command-r-08-2024': 992.6319053544709,\n",
       " 'mistral-saba': 988.3173081520897,\n",
       " 'llama-3.1-70b': 981.0959732516199,\n",
       " 'llama-3.1-405b': 977.8371399839351,\n",
       " 'gemma-2-27b-it-q8': 972.9039034682424,\n",
       " 'aya-expanse-8b': 971.8167439533642,\n",
       " 'lfm-40b': 966.8775201138988,\n",
       " 'mistral-small-24b-instruct-2501': 958.3581933285648,\n",
       " 'jamba-1.5-large': 954.9330637347991,\n",
       " 'qwen2.5-coder-32b-instruct': 942.2499395325128,\n",
       " 'o3-mini': 941.8837486762151,\n",
       " 'qwen2.5-7b-instruct': 932.1394504525656,\n",
       " 'o4-mini': 929.3059647968744,\n",
       " 'mistral-nemo-2407': 925.059630464839,\n",
       " 'deepseek-r1-distill-llama-70b': 922.7646088004693,\n",
       " 'llama-3.3-70b': 920.7874109940955,\n",
       " 'hermes-3-llama-3.1-405b': 919.4951311945364,\n",
       " 'llama-3.1-8b': 911.3066446743362,\n",
       " 'phi-4': 907.8272991447487,\n",
       " 'mixtral-8x22b-instruct-v0.1': 886.9488960122875,\n",
       " 'gpt-4o-mini-2024-07-18': 882.9951251885235,\n",
       " 'phi-3.5-mini-instruct': 859.9439075250559,\n",
       " 'ministral-8b-instruct-2410': 846.8970521144353,\n",
       " 'gemma-2-9b-it': 804.9235612693838,\n",
       " 'chocolatine-2-14b-instruct-v2.0.3-q8': 799.7454624733351}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker = ELORanker(K=40)\n",
    "\n",
    "random.seed(1337)\n",
    "matches = random.sample(matches, k=len(matches))\n",
    "ranker.add_players(model_names)  # type: ignore\n",
    "ranker.compute_scores(matches=matches)\n",
    "ranker.get_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul d'un score de frugalité\n",
    "\n",
    "Le score de frugalité est calculé à partir de données de consommation présentes dans le jeu de données `comparia-conversations`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul du nombre de match et du nombre total de tokens générés par modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (48, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_name</th><th>n_match</th><th>total_output_tokens</th></tr><tr><td>str</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;aya-expanse-8b&quot;</td><td>519</td><td>961445.0</td></tr><tr><td>&quot;c4ai-command-r-08-2024&quot;</td><td>1180</td><td>5.944341e6</td></tr><tr><td>&quot;chocolatine-2-14b-instruct-v2.…</td><td>598</td><td>511086.0</td></tr><tr><td>&quot;claude-3-5-sonnet-v2&quot;</td><td>1994</td><td>2.818225e6</td></tr><tr><td>&quot;claude-3-7-sonnet&quot;</td><td>319</td><td>2.162331e6</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;phi-3.5-mini-instruct&quot;</td><td>462</td><td>851005.0</td></tr><tr><td>&quot;phi-4&quot;</td><td>1616</td><td>3.086417e6</td></tr><tr><td>&quot;qwen2.5-7b-instruct&quot;</td><td>427</td><td>870902.0</td></tr><tr><td>&quot;qwen2.5-coder-32b-instruct&quot;</td><td>1641</td><td>4.787749e6</td></tr><tr><td>&quot;qwq-32b&quot;</td><td>321</td><td>946986.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (48, 3)\n",
       "┌─────────────────────────────────┬─────────┬─────────────────────┐\n",
       "│ model_name                      ┆ n_match ┆ total_output_tokens │\n",
       "│ ---                             ┆ ---     ┆ ---                 │\n",
       "│ str                             ┆ i64     ┆ f64                 │\n",
       "╞═════════════════════════════════╪═════════╪═════════════════════╡\n",
       "│ aya-expanse-8b                  ┆ 519     ┆ 961445.0            │\n",
       "│ c4ai-command-r-08-2024          ┆ 1180    ┆ 5.944341e6          │\n",
       "│ chocolatine-2-14b-instruct-v2.… ┆ 598     ┆ 511086.0            │\n",
       "│ claude-3-5-sonnet-v2            ┆ 1994    ┆ 2.818225e6          │\n",
       "│ claude-3-7-sonnet               ┆ 319     ┆ 2.162331e6          │\n",
       "│ …                               ┆ …       ┆ …                   │\n",
       "│ phi-3.5-mini-instruct           ┆ 462     ┆ 851005.0            │\n",
       "│ phi-4                           ┆ 1616    ┆ 3.086417e6          │\n",
       "│ qwen2.5-7b-instruct             ┆ 427     ┆ 870902.0            │\n",
       "│ qwen2.5-coder-32b-instruct      ┆ 1641    ┆ 4.787749e6          │\n",
       "│ qwq-32b                         ┆ 321     ┆ 946986.0            │\n",
       "└─────────────────────────────────┴─────────┴─────────────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "from rank_comparia.frugality import get_n_match, get_models_output_tokens\n",
    "\n",
    "number_by_model = get_n_match(ranker)\n",
    "total_tokens = get_models_output_tokens(reactions)\n",
    "\n",
    "number_by_model = number_by_model.join(total_tokens, on=\"model_name\")\n",
    "\n",
    "number_by_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul du score de frugalité\n",
    "\n",
    "Calcul du score énergétique. Il est possible de moyenner les scores avec le paramètre `mean` (si True, le score est moyenné, sinon non).  \n",
    "Si on décide de moyenner, le moyennage par tokens et par nombre de match est effectué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (48, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_name</th><th>total_output_tokens</th><th>conso_all_conv</th><th>n_match</th><th>total_output_tokens_right</th><th>mean_conso_per_match</th><th>mean_conso_per_token</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;aya-expanse-8b&quot;</td><td>961445.0</td><td>3.62259</td><td>519</td><td>961445.0</td><td>0.00698</td><td>0.000004</td></tr><tr><td>&quot;c4ai-command-r-08-2024&quot;</td><td>5.944341e6</td><td>44.921088</td><td>1180</td><td>5.944341e6</td><td>0.038069</td><td>0.000008</td></tr><tr><td>&quot;chocolatine-2-14b-instruct-v2.…</td><td>511086.0</td><td>1.853976</td><td>598</td><td>511086.0</td><td>0.0031</td><td>0.000004</td></tr><tr><td>&quot;claude-3-5-sonnet-v2&quot;</td><td>2.818225e6</td><td>378.314297</td><td>1994</td><td>2.818225e6</td><td>0.189726</td><td>0.000134</td></tr><tr><td>&quot;claude-3-7-sonnet&quot;</td><td>2.162331e6</td><td>290.26807</td><td>319</td><td>2.162331e6</td><td>0.909931</td><td>0.000134</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;phi-3.5-mini-instruct&quot;</td><td>851005.0</td><td>2.609332</td><td>462</td><td>851005.0</td><td>0.005648</td><td>0.000003</td></tr><tr><td>&quot;phi-4&quot;</td><td>3.086417e6</td><td>14.228012</td><td>1616</td><td>3.086417e6</td><td>0.008804</td><td>0.000005</td></tr><tr><td>&quot;qwen2.5-7b-instruct&quot;</td><td>870902.0</td><td>3.159217</td><td>427</td><td>870902.0</td><td>0.007399</td><td>0.000004</td></tr><tr><td>&quot;qwen2.5-coder-32b-instruct&quot;</td><td>4.787749e6</td><td>34.16509</td><td>1641</td><td>4.787749e6</td><td>0.02082</td><td>0.000007</td></tr><tr><td>&quot;qwq-32b&quot;</td><td>946986.0</td><td>6.757635</td><td>321</td><td>946986.0</td><td>0.021052</td><td>0.000007</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (48, 7)\n",
       "┌──────────────┬──────────────┬──────────────┬─────────┬──────────────┬──────────────┬─────────────┐\n",
       "│ model_name   ┆ total_output ┆ conso_all_co ┆ n_match ┆ total_output ┆ mean_conso_p ┆ mean_conso_ │\n",
       "│ ---          ┆ _tokens      ┆ nv           ┆ ---     ┆ _tokens_righ ┆ er_match     ┆ per_token   │\n",
       "│ str          ┆ ---          ┆ ---          ┆ i64     ┆ t            ┆ ---          ┆ ---         │\n",
       "│              ┆ f64          ┆ f64          ┆         ┆ ---          ┆ f64          ┆ f64         │\n",
       "│              ┆              ┆              ┆         ┆ f64          ┆              ┆             │\n",
       "╞══════════════╪══════════════╪══════════════╪═════════╪══════════════╪══════════════╪═════════════╡\n",
       "│ aya-expanse- ┆ 961445.0     ┆ 3.62259      ┆ 519     ┆ 961445.0     ┆ 0.00698      ┆ 0.000004    │\n",
       "│ 8b           ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ c4ai-command ┆ 5.944341e6   ┆ 44.921088    ┆ 1180    ┆ 5.944341e6   ┆ 0.038069     ┆ 0.000008    │\n",
       "│ -r-08-2024   ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ chocolatine- ┆ 511086.0     ┆ 1.853976     ┆ 598     ┆ 511086.0     ┆ 0.0031       ┆ 0.000004    │\n",
       "│ 2-14b-instru ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ ct-v2.…      ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ claude-3-5-s ┆ 2.818225e6   ┆ 378.314297   ┆ 1994    ┆ 2.818225e6   ┆ 0.189726     ┆ 0.000134    │\n",
       "│ onnet-v2     ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ claude-3-7-s ┆ 2.162331e6   ┆ 290.26807    ┆ 319     ┆ 2.162331e6   ┆ 0.909931     ┆ 0.000134    │\n",
       "│ onnet        ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ …            ┆ …            ┆ …            ┆ …       ┆ …            ┆ …            ┆ …           │\n",
       "│ phi-3.5-mini ┆ 851005.0     ┆ 2.609332     ┆ 462     ┆ 851005.0     ┆ 0.005648     ┆ 0.000003    │\n",
       "│ -instruct    ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ phi-4        ┆ 3.086417e6   ┆ 14.228012    ┆ 1616    ┆ 3.086417e6   ┆ 0.008804     ┆ 0.000005    │\n",
       "│ qwen2.5-7b-i ┆ 870902.0     ┆ 3.159217     ┆ 427     ┆ 870902.0     ┆ 0.007399     ┆ 0.000004    │\n",
       "│ nstruct      ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ qwen2.5-code ┆ 4.787749e6   ┆ 34.16509     ┆ 1641    ┆ 4.787749e6   ┆ 0.02082      ┆ 0.000007    │\n",
       "│ r-32b-instru ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ ct           ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ qwq-32b      ┆ 946986.0     ┆ 6.757635     ┆ 321     ┆ 946986.0     ┆ 0.021052     ┆ 0.000007    │\n",
       "└──────────────┴──────────────┴──────────────┴─────────┴──────────────┴──────────────┴─────────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rank_comparia.frugality import calculate_frugality_score, draw_chart\n",
    "\n",
    "frugal_scores = calculate_frugality_score(reactions, number_by_model, mean=True)\n",
    "\n",
    "frugal_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (48, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_name</th><th>elo_score</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;gemma-3-27b&quot;</td><td>1191.339501</td></tr><tr><td>&quot;gemini-2.0-flash-exp&quot;</td><td>1144.620434</td></tr><tr><td>&quot;gpt-4.1-mini&quot;</td><td>1136.981015</td></tr><tr><td>&quot;claude-3-7-sonnet&quot;</td><td>1136.972711</td></tr><tr><td>&quot;llama-3.1-nemotron-70b-instruc…</td><td>1129.535116</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;gpt-4o-mini-2024-07-18&quot;</td><td>882.995125</td></tr><tr><td>&quot;phi-3.5-mini-instruct&quot;</td><td>859.943908</td></tr><tr><td>&quot;ministral-8b-instruct-2410&quot;</td><td>846.897052</td></tr><tr><td>&quot;gemma-2-9b-it&quot;</td><td>804.923561</td></tr><tr><td>&quot;chocolatine-2-14b-instruct-v2.…</td><td>799.745462</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (48, 2)\n",
       "┌─────────────────────────────────┬─────────────┐\n",
       "│ model_name                      ┆ elo_score   │\n",
       "│ ---                             ┆ ---         │\n",
       "│ str                             ┆ f64         │\n",
       "╞═════════════════════════════════╪═════════════╡\n",
       "│ gemma-3-27b                     ┆ 1191.339501 │\n",
       "│ gemini-2.0-flash-exp            ┆ 1144.620434 │\n",
       "│ gpt-4.1-mini                    ┆ 1136.981015 │\n",
       "│ claude-3-7-sonnet               ┆ 1136.972711 │\n",
       "│ llama-3.1-nemotron-70b-instruc… ┆ 1129.535116 │\n",
       "│ …                               ┆ …           │\n",
       "│ gpt-4o-mini-2024-07-18          ┆ 882.995125  │\n",
       "│ phi-3.5-mini-instruct           ┆ 859.943908  │\n",
       "│ ministral-8b-instruct-2410      ┆ 846.897052  │\n",
       "│ gemma-2-9b-it                   ┆ 804.923561  │\n",
       "│ chocolatine-2-14b-instruct-v2.… ┆ 799.745462  │\n",
       "└─────────────────────────────────┴─────────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elo_scores = pl.DataFrame(\n",
    "    {\n",
    "        \"model_name\": ranker.players.keys(),\n",
    "        \"elo_score\": ranker.players.values(),\n",
    "    },\n",
    "    strict=False,\n",
    ").sort(by=\"elo_score\", descending=True)\n",
    "\n",
    "elo_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du graphique de frugalité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des informations concernant les modèles du comparateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "info_model = pl.read_json(source=Path(\".\").resolve().parent / \"data\" / \"models_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (39, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>name</th><th>model_name</th><th>organization</th><th>license</th><th>elo_score</th><th>total_output_tokens</th><th>conso_all_conv</th><th>n_match</th><th>total_output_tokens_right</th><th>mean_conso_per_match</th><th>mean_conso_per_token</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Aya-Expanse-8B&quot;</td><td>&quot;aya-expanse-8b&quot;</td><td>&quot;Cohere&quot;</td><td>&quot;CC-BY-NC-4.0&quot;</td><td>971.816744</td><td>961445.0</td><td>3.62259</td><td>519</td><td>961445.0</td><td>0.00698</td><td>0.000004</td></tr><tr><td>&quot;Command R (08-2024)&quot;</td><td>&quot;c4ai-command-r-08-2024&quot;</td><td>&quot;Cohere&quot;</td><td>&quot;CC-BY-NC-4.0&quot;</td><td>992.631905</td><td>5.944341e6</td><td>44.921088</td><td>1180</td><td>5.944341e6</td><td>0.038069</td><td>0.000008</td></tr><tr><td>&quot;Chocolatine-2-14b Instruct&quot;</td><td>&quot;chocolatine-2-14b-instruct-v2.…</td><td>&quot;jpacifico (individual)&quot;</td><td>&quot;Apache 2.0&quot;</td><td>799.745462</td><td>511086.0</td><td>1.853976</td><td>598</td><td>511086.0</td><td>0.0031</td><td>0.000004</td></tr><tr><td>&quot;Claude 3.5 Sonnet V2&quot;</td><td>&quot;claude-3-5-sonnet-v2&quot;</td><td>&quot;Anthropic&quot;</td><td>&quot;Proprietary&quot;</td><td>1061.522493</td><td>2.818225e6</td><td>378.314297</td><td>1994</td><td>2.818225e6</td><td>0.189726</td><td>0.000134</td></tr><tr><td>&quot;Command A&quot;</td><td>&quot;command-a&quot;</td><td>&quot;Cohere&quot;</td><td>&quot;CC-BY-NC-4.0&quot;</td><td>1124.262695</td><td>1.03253e6</td><td>18.815316</td><td>457</td><td>1.03253e6</td><td>0.041171</td><td>0.000018</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;Phi-3.5 Mini Instruct&quot;</td><td>&quot;phi-3.5-mini-instruct&quot;</td><td>&quot;Microsoft&quot;</td><td>&quot;MIT&quot;</td><td>859.943908</td><td>851005.0</td><td>2.609332</td><td>462</td><td>851005.0</td><td>0.005648</td><td>0.000003</td></tr><tr><td>&quot;Phi 4&quot;</td><td>&quot;phi-4&quot;</td><td>&quot;Microsoft&quot;</td><td>&quot;MIT&quot;</td><td>907.827299</td><td>3.086417e6</td><td>14.228012</td><td>1616</td><td>3.086417e6</td><td>0.008804</td><td>0.000005</td></tr><tr><td>&quot;Qwen2.5-7B&quot;</td><td>&quot;qwen2.5-7b-instruct&quot;</td><td>&quot;Alibaba&quot;</td><td>&quot;Apache 2.0&quot;</td><td>932.13945</td><td>870902.0</td><td>3.159217</td><td>427</td><td>870902.0</td><td>0.007399</td><td>0.000004</td></tr><tr><td>&quot;Qwen2.5-Coder-32B-Instruct&quot;</td><td>&quot;qwen2.5-coder-32b-instruct&quot;</td><td>&quot;Alibaba&quot;</td><td>&quot;Apache 2.0&quot;</td><td>942.24994</td><td>4.787749e6</td><td>34.16509</td><td>1641</td><td>4.787749e6</td><td>0.02082</td><td>0.000007</td></tr><tr><td>&quot;QwQ 32B&quot;</td><td>&quot;qwq-32b&quot;</td><td>&quot;Alibaba&quot;</td><td>&quot;Apache 2.0&quot;</td><td>1024.59679</td><td>946986.0</td><td>6.757635</td><td>321</td><td>946986.0</td><td>0.021052</td><td>0.000007</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (39, 11)\n",
       "┌────────────┬───────────┬───────────┬───────────┬───┬─────────┬───────────┬───────────┬───────────┐\n",
       "│ name       ┆ model_nam ┆ organizat ┆ license   ┆ … ┆ n_match ┆ total_out ┆ mean_cons ┆ mean_cons │\n",
       "│ ---        ┆ e         ┆ ion       ┆ ---       ┆   ┆ ---     ┆ put_token ┆ o_per_mat ┆ o_per_tok │\n",
       "│ str        ┆ ---       ┆ ---       ┆ str       ┆   ┆ i64     ┆ s_right   ┆ ch        ┆ en        │\n",
       "│            ┆ str       ┆ str       ┆           ┆   ┆         ┆ ---       ┆ ---       ┆ ---       │\n",
       "│            ┆           ┆           ┆           ┆   ┆         ┆ f64       ┆ f64       ┆ f64       │\n",
       "╞════════════╪═══════════╪═══════════╪═══════════╪═══╪═════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ Aya-Expans ┆ aya-expan ┆ Cohere    ┆ CC-BY-NC- ┆ … ┆ 519     ┆ 961445.0  ┆ 0.00698   ┆ 0.000004  │\n",
       "│ e-8B       ┆ se-8b     ┆           ┆ 4.0       ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Command R  ┆ c4ai-comm ┆ Cohere    ┆ CC-BY-NC- ┆ … ┆ 1180    ┆ 5.944341e ┆ 0.038069  ┆ 0.000008  │\n",
       "│ (08-2024)  ┆ and-r-08- ┆           ┆ 4.0       ┆   ┆         ┆ 6         ┆           ┆           │\n",
       "│            ┆ 2024      ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Chocolatin ┆ chocolati ┆ jpacifico ┆ Apache    ┆ … ┆ 598     ┆ 511086.0  ┆ 0.0031    ┆ 0.000004  │\n",
       "│ e-2-14b    ┆ ne-2-14b- ┆ (individu ┆ 2.0       ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Instruct   ┆ instruct- ┆ al)       ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│            ┆ v2.…      ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Claude 3.5 ┆ claude-3- ┆ Anthropic ┆ Proprieta ┆ … ┆ 1994    ┆ 2.818225e ┆ 0.189726  ┆ 0.000134  │\n",
       "│ Sonnet V2  ┆ 5-sonnet- ┆           ┆ ry        ┆   ┆         ┆ 6         ┆           ┆           │\n",
       "│            ┆ v2        ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Command A  ┆ command-a ┆ Cohere    ┆ CC-BY-NC- ┆ … ┆ 457     ┆ 1.03253e6 ┆ 0.041171  ┆ 0.000018  │\n",
       "│            ┆           ┆           ┆ 4.0       ┆   ┆         ┆           ┆           ┆           │\n",
       "│ …          ┆ …         ┆ …         ┆ …         ┆ … ┆ …       ┆ …         ┆ …         ┆ …         │\n",
       "│ Phi-3.5    ┆ phi-3.5-m ┆ Microsoft ┆ MIT       ┆ … ┆ 462     ┆ 851005.0  ┆ 0.005648  ┆ 0.000003  │\n",
       "│ Mini       ┆ ini-instr ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Instruct   ┆ uct       ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Phi 4      ┆ phi-4     ┆ Microsoft ┆ MIT       ┆ … ┆ 1616    ┆ 3.086417e ┆ 0.008804  ┆ 0.000005  │\n",
       "│            ┆           ┆           ┆           ┆   ┆         ┆ 6         ┆           ┆           │\n",
       "│ Qwen2.5-7B ┆ qwen2.5-7 ┆ Alibaba   ┆ Apache    ┆ … ┆ 427     ┆ 870902.0  ┆ 0.007399  ┆ 0.000004  │\n",
       "│            ┆ b-instruc ┆           ┆ 2.0       ┆   ┆         ┆           ┆           ┆           │\n",
       "│            ┆ t         ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Qwen2.5-Co ┆ qwen2.5-c ┆ Alibaba   ┆ Apache    ┆ … ┆ 1641    ┆ 4.787749e ┆ 0.02082   ┆ 0.000007  │\n",
       "│ der-32B-In ┆ oder-32b- ┆           ┆ 2.0       ┆   ┆         ┆ 6         ┆           ┆           │\n",
       "│ struct     ┆ instruct  ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ QwQ 32B    ┆ qwq-32b   ┆ Alibaba   ┆ Apache    ┆ … ┆ 321     ┆ 946986.0  ┆ 0.021052  ┆ 0.000007  │\n",
       "│            ┆           ┆           ┆ 2.0       ┆   ┆         ┆           ┆           ┆           │\n",
       "└────────────┴───────────┴───────────┴───────────┴───┴─────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = info_model.join(elo_scores, on=\"model_name\").join(frugal_scores, on=\"model_name\")\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_comparia.utils import save_data\n",
    "\n",
    "save_path = Path(\".\").resolve().parent / \"data\"\n",
    "save_data(final_df, \"all_info_for_chart_drawing\", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération du graphique de frugalité\n",
    "\n",
    "Les paramètres possibles :  \n",
    "- `log` : Ajuster l'échelle du graphique en linéaire (`log = False`) ou en log (`log = True`) ; \n",
    "- `mean` : Utiliser les consommation moyenné (`mean = True`) ou non (`mean = False`) ;  \n",
    "- `scale` :  choix du moyennage si `mean = True`. `token` si on utilise le moyennage par token, `match` si on utilise le moyennage par nombre de match ;  \n",
    "- `save` : Enregistrement du graphique au format html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-6c628c424e714633a94e7a144060efd4.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-6c628c424e714633a94e7a144060efd4.vega-embed details,\n",
       "  #altair-viz-6c628c424e714633a94e7a144060efd4.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-6c628c424e714633a94e7a144060efd4\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-6c628c424e714633a94e7a144060efd4\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-6c628c424e714633a94e7a144060efd4\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-995f201a7b549cd86d64c9242a0aaaf4\"}, \"mark\": {\"type\": \"point\", \"filled\": true}, \"encoding\": {\"color\": {\"field\": \"organization\", \"type\": \"nominal\"}, \"opacity\": {\"condition\": [{\"param\": \"param_4\", \"value\": 1}], \"value\": 0.3}, \"tooltip\": [{\"field\": \"model_name\", \"type\": \"nominal\"}, {\"field\": \"organization\", \"type\": \"nominal\"}, {\"field\": \"license\", \"type\": \"nominal\"}, {\"field\": \"elo_score\", \"type\": \"quantitative\"}, {\"field\": \"conso_all_conv\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"conso_all_conv\", \"scale\": {\"type\": \"log\"}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"elo_score\", \"scale\": {\"zero\": false}, \"type\": \"quantitative\"}}, \"height\": 300, \"params\": [{\"name\": \"param_4\", \"select\": {\"type\": \"point\", \"fields\": [\"organization\"]}, \"bind\": \"legend\"}, {\"name\": \"param_3\", \"select\": {\"type\": \"point\", \"fields\": [\"license\"]}, \"bind\": {\"input\": \"select\", \"options\": [\"Proprietary\", \"DeepSeek\", \"Llama 3.3 Community\", \"MIT\", \"Gemma license\", \"CC-BY-NC-4.0\", \"Gemma\", \"llama3\", \"Apache 2.0\", \"Mistral Research\", \"MRL\", \"Jamba Open Model License\", \"Llama 3.1 Community\", \"Unknown\"], \"labels\": [\"Mistral Research\", \"Llama 3.3 Community\", \"DeepSeek\", \"MRL\", \"Jamba Open Model License\", \"CC-BY-NC-4.0\", \"llama3\", \"Gemma license\", \"MIT\", \"Proprietary\", \"Apache 2.0\", \"Gemma\", \"Unknown\", \"Llama 3.1 Community\"], \"name\": \"License : \"}}], \"title\": \"consommation selon classement\", \"transform\": [{\"filter\": {\"param\": \"param_3\"}}], \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-995f201a7b549cd86d64c9242a0aaaf4\": [{\"name\": \"Aya-Expanse-8B\", \"model_name\": \"aya-expanse-8b\", \"organization\": \"Cohere\", \"license\": \"CC-BY-NC-4.0\", \"elo_score\": 971.8167439533642, \"total_output_tokens\": 961445.0, \"conso_all_conv\": 3.6225901576999946, \"n_match\": 519, \"total_output_tokens_right\": 961445.0, \"mean_conso_per_match\": 0.006979942500385346, \"mean_conso_per_token\": 3.767859999999994e-06}, {\"name\": \"Command R (08-2024)\", \"model_name\": \"c4ai-command-r-08-2024\", \"organization\": \"Cohere\", \"license\": \"CC-BY-NC-4.0\", \"elo_score\": 992.6319053544709, \"total_output_tokens\": 5944341.0, \"conso_all_conv\": 44.92108771994969, \"n_match\": 1180, \"total_output_tokens_right\": 5944341.0, \"mean_conso_per_match\": 0.03806871840673703, \"mean_conso_per_token\": 7.556949999999948e-06}, {\"name\": \"Chocolatine-2-14b Instruct\", \"model_name\": \"chocolatine-2-14b-instruct-v2.0.3-q8\", \"organization\": \"jpacifico (individual)\", \"license\": \"Apache 2.0\", \"elo_score\": 799.7454624733351, \"total_output_tokens\": 511086.0, \"conso_all_conv\": 1.853976390340002, \"n_match\": 598, \"total_output_tokens_right\": 511086.0, \"mean_conso_per_match\": 0.003100294967123749, \"mean_conso_per_token\": 3.627523333333337e-06}, {\"name\": \"Claude 3.5 Sonnet V2\", \"model_name\": \"claude-3-5-sonnet-v2\", \"organization\": \"Anthropic\", \"license\": \"Proprietary\", \"elo_score\": 1061.5224933944878, \"total_output_tokens\": 2818225.0, \"conso_all_conv\": 378.31429666249966, \"n_match\": 1994, \"total_output_tokens_right\": 2818225.0, \"mean_conso_per_match\": 0.1897263273131894, \"mean_conso_per_token\": 0.00013423849999999988}, {\"name\": \"Command A\", \"model_name\": \"command-a\", \"organization\": \"Cohere\", \"license\": \"CC-BY-NC-4.0\", \"elo_score\": 1124.262695420955, \"total_output_tokens\": 1032530.0, \"conso_all_conv\": 18.815315784433313, \"n_match\": 457, \"total_output_tokens_right\": 1032530.0, \"mean_conso_per_match\": 0.04117136933136392, \"mean_conso_per_token\": 1.8222536666666648e-05}, {\"name\": \"DeepSeek-R1\", \"model_name\": \"deepseek-r1\", \"organization\": \"DeepSeek\", \"license\": \"MIT\", \"elo_score\": 1008.2185359046271, \"total_output_tokens\": 1160060.0, \"conso_all_conv\": 54.55267994439995, \"n_match\": 486, \"total_output_tokens_right\": 1160060.0, \"mean_conso_per_match\": 0.11224831264279825, \"mean_conso_per_token\": 4.7025739999999955e-05}, {\"name\": \"DeepSeek-R1 distiil\", \"model_name\": \"deepseek-r1-distill-llama-70b\", \"organization\": \"DeepSeek\", \"license\": \"MIT\", \"elo_score\": 922.7646088004693, \"total_output_tokens\": 741901.0, \"conso_all_conv\": 9.250565728733326, \"n_match\": 454, \"total_output_tokens_right\": 741901.0, \"mean_conso_per_match\": 0.020375695437738605, \"mean_conso_per_token\": 1.2468733333333323e-05}, {\"name\": \"DeepSeek-V3 Chat\", \"model_name\": \"deepseek-v3-chat\", \"organization\": \"DeepSeek\", \"license\": \"DeepSeek\", \"elo_score\": 1055.8583879004634, \"total_output_tokens\": 9475940.0, \"conso_all_conv\": 445.6130906956012, \"n_match\": 1866, \"total_output_tokens_right\": 9475940.0, \"mean_conso_per_match\": 0.23880658665359122, \"mean_conso_per_token\": 4.702574000000013e-05}, {\"name\": \"Gemini 1.5 pro 001\", \"model_name\": \"gemini-1.5-pro-001\", \"organization\": \"Google\", \"license\": \"Proprietary\", \"elo_score\": 1041.22455039952, \"total_output_tokens\": 612722.0, \"conso_all_conv\": 82.15188674586669, \"n_match\": 392, \"total_output_tokens_right\": 612722.0, \"mean_conso_per_match\": 0.2095711396578232, \"mean_conso_per_token\": 0.00013407693333333337}, {\"name\": \"Gemini 1.5 pro 002\", \"model_name\": \"gemini-1.5-pro-002\", \"organization\": \"Google\", \"license\": \"Proprietary\", \"elo_score\": 1094.2971057766233, \"total_output_tokens\": 3284077.0, \"conso_all_conv\": 440.3189729905331, \"n_match\": 1672, \"total_output_tokens_right\": 3284077.0, \"mean_conso_per_match\": 0.26334866805653895, \"mean_conso_per_token\": 0.00013407693333333326}, {\"name\": \"Gemini 2.0 Flash\", \"model_name\": \"gemini-2.0-flash-001\", \"organization\": \"Google\", \"license\": \"Proprietary\", \"elo_score\": 1096.177320994962, \"total_output_tokens\": 2300660.0, \"conso_all_conv\": 19.000307364666675, \"n_match\": 806, \"total_output_tokens_right\": 2300660.0, \"mean_conso_per_match\": 0.023573582338296123, \"mean_conso_per_token\": 8.258633333333338e-06}, {\"name\": \"Gemini 2.0 Flash\", \"model_name\": \"gemini-2.0-flash-exp\", \"organization\": \"Google\", \"license\": \"Proprietary\", \"elo_score\": 1144.6204338742177, \"total_output_tokens\": 3076613.0, \"conso_all_conv\": 25.408618675566665, \"n_match\": 1001, \"total_output_tokens_right\": 3076613.0, \"mean_conso_per_match\": 0.025383235440126536, \"mean_conso_per_token\": 8.258633333333333e-06}, {\"name\": \"Gemma 2 27B q8\", \"model_name\": \"gemma-2-27b-it-q8\", \"organization\": \"Google\", \"license\": \"Gemma license\", \"elo_score\": 972.9039034682424, \"total_output_tokens\": 242100.0, \"conso_all_conv\": 1.5577335389999991, \"n_match\": 170, \"total_output_tokens_right\": 242100.0, \"mean_conso_per_match\": 0.009163138464705877, \"mean_conso_per_token\": 6.434256666666663e-06}, {\"name\": \"Gemma 2 9B\", \"model_name\": \"gemma-2-9b-it\", \"organization\": \"Google\", \"license\": \"Gemma license\", \"elo_score\": 804.9235612693838, \"total_output_tokens\": 2016207.0, \"conso_all_conv\": 7.879733476710017, \"n_match\": 1520, \"total_output_tokens_right\": 2016207.0, \"mean_conso_per_match\": 0.005184035182046064, \"mean_conso_per_token\": 3.908196666666675e-06}, {\"name\": \"Gemma 3 12B\", \"model_name\": \"gemma-3-12b\", \"organization\": \"Google\", \"license\": \"Gemma license\", \"elo_score\": 1110.973006187927, \"total_output_tokens\": 1088635.0, \"conso_all_conv\": 4.712925899566667, \"n_match\": 457, \"total_output_tokens_right\": 1088635.0, \"mean_conso_per_match\": 0.010312748139095551, \"mean_conso_per_token\": 4.329206666666667e-06}, {\"name\": \"Gemma-3 27B\", \"model_name\": \"gemma-3-27b\", \"organization\": \"Google\", \"license\": \"Gemma\", \"elo_score\": 1191.3395006565374, \"total_output_tokens\": 1405974.0, \"conso_all_conv\": 9.046397582660008, \"n_match\": 579, \"total_output_tokens_right\": 1405974.0, \"mean_conso_per_match\": 0.01562417544500865, \"mean_conso_per_token\": 6.434256666666672e-06}, {\"name\": \"Gemma 3 4B\", \"model_name\": \"gemma-3-4b\", \"organization\": \"Google\", \"license\": \"Gemma license\", \"elo_score\": 1067.4333848739445, \"total_output_tokens\": 1147829.0, \"conso_all_conv\": 3.6805289928866673, \"n_match\": 459, \"total_output_tokens_right\": 1147829.0, \"mean_conso_per_match\": 0.008018581683848948, \"mean_conso_per_token\": 3.206513333333334e-06}, {\"name\": \"GPT-4o mini\", \"model_name\": \"gpt-4o-mini-2024-07-18\", \"organization\": \"OpenAI\", \"license\": \"Proprietary\", \"elo_score\": 882.9951251885235, \"total_output_tokens\": 3805215.0, \"conso_all_conv\": 28.75581949424996, \"n_match\": 2183, \"total_output_tokens_right\": 3805215.0, \"mean_conso_per_match\": 0.013172615434837361, \"mean_conso_per_token\": 7.5569499999999895e-06}, {\"name\": \"Hermes 3\", \"model_name\": \"hermes-3-llama-3.1-405b\", \"organization\": \"Nous Research\", \"license\": \"llama3\", \"elo_score\": 919.4951311945364, \"total_output_tokens\": 4182418.0, \"conso_all_conv\": 995.1062638958654, \"n_match\": 1381, \"total_output_tokens_right\": 4182418.0, \"mean_conso_per_match\": 0.7205693438782516, \"mean_conso_per_token\": 0.00023792606666666637}, {\"name\": \"Jamba Large\", \"model_name\": \"jamba-1.5-large\", \"organization\": \"AI21 Labs\", \"license\": \"Jamba Open Model License\", \"elo_score\": 954.9330637347991, \"total_output_tokens\": 581721.0, \"conso_all_conv\": 27.637820667239993, \"n_match\": 77, \"total_output_tokens_right\": 581721.0, \"mean_conso_per_match\": 0.3589327359381817, \"mean_conso_per_token\": 4.751043999999999e-05}, {\"name\": \"Liquid Foundation Model\", \"model_name\": \"lfm-40b\", \"organization\": \"Liquid\", \"license\": \"Unknown\", \"elo_score\": 966.8775201138988, \"total_output_tokens\": 1933376.0, \"conso_all_conv\": 15.96704347946668, \"n_match\": 1069, \"total_output_tokens_right\": 1933376.0, \"mean_conso_per_match\": 0.014936429821764902, \"mean_conso_per_token\": 8.258633333333341e-06}, {\"name\": \"Llama 3.1 405B\", \"model_name\": \"llama-3.1-405b\", \"organization\": \"Meta\", \"license\": \"Llama 3.1 Community\", \"elo_score\": 977.8371399839351, \"total_output_tokens\": 4276508.0, \"conso_all_conv\": 1017.4927275085383, \"n_match\": 2114, \"total_output_tokens_right\": 4276508.0, \"mean_conso_per_match\": 0.4813116024165271, \"mean_conso_per_token\": 0.00023792606666666783}, {\"name\": \"Llama-3.1-70B\", \"model_name\": \"llama-3.1-70b\", \"organization\": \"Meta\", \"license\": \"Llama 3.1 Community\", \"elo_score\": 981.0959732516199, \"total_output_tokens\": 2450777.0, \"conso_all_conv\": 30.55808487246673, \"n_match\": 1597, \"total_output_tokens_right\": 2450777.0, \"mean_conso_per_match\": 0.01913468057136301, \"mean_conso_per_token\": 1.2468733333333359e-05}, {\"name\": \"Llama 3.1 8B\", \"model_name\": \"llama-3.1-8b\", \"organization\": \"Meta\", \"license\": \"Llama 3.1 Community\", \"elo_score\": 911.3066446743362, \"total_output_tokens\": 2993533.0, \"conso_all_conv\": 11.279213249379985, \"n_match\": 1785, \"total_output_tokens_right\": 2993533.0, \"mean_conso_per_match\": 0.006318886974442569, \"mean_conso_per_token\": 3.767859999999995e-06}, {\"name\": \"Nemotron 70B Instruct\", \"model_name\": \"llama-3.1-nemotron-70b-instruct\", \"organization\": \"Nvidia\", \"license\": \"Llama 3.1 Community\", \"elo_score\": 1129.535115932537, \"total_output_tokens\": 3957200.0, \"conso_all_conv\": 49.341271546666775, \"n_match\": 1269, \"total_output_tokens_right\": 3957200.0, \"mean_conso_per_match\": 0.03888201067507232, \"mean_conso_per_token\": 1.246873333333336e-05}, {\"name\": \"Llama 3.3 70B\", \"model_name\": \"llama-3.3-70b\", \"organization\": \"Meta\", \"license\": \"Llama 3.3 Community\", \"elo_score\": 920.7874109940955, \"total_output_tokens\": 5793973.0, \"conso_all_conv\": 72.24350427753315, \"n_match\": 1207, \"total_output_tokens_right\": 5793973.0, \"mean_conso_per_match\": 0.059853773220822824, \"mean_conso_per_token\": 1.2468733333333301e-05}, {\"name\": \"Ministral 8B-Instruct\", \"model_name\": \"ministral-8b-instruct-2410\", \"organization\": \"Mistral\", \"license\": \"MRL\", \"elo_score\": 846.8970521144353, \"total_output_tokens\": 3227726.0, \"conso_all_conv\": 12.161619686360076, \"n_match\": 1962, \"total_output_tokens_right\": 3227726.0, \"mean_conso_per_match\": 0.006198582918634086, \"mean_conso_per_token\": 3.767860000000024e-06}, {\"name\": \"Mistral-Large-2411\", \"model_name\": \"mistral-large-2411\", \"organization\": \"Mistral\", \"license\": \"Mistral Research\", \"elo_score\": 1079.1626953808234, \"total_output_tokens\": 9064145.0, \"conso_all_conv\": 180.43609736028264, \"n_match\": 1849, \"total_output_tokens_right\": 9064145.0, \"mean_conso_per_match\": 0.09758577466754063, \"mean_conso_per_token\": 1.990657666666659e-05}, {\"name\": \"Mistral Nemo Instruct\", \"model_name\": \"mistral-nemo-2407\", \"organization\": \"Mistral AI\", \"license\": \"Apache 2.0\", \"elo_score\": 925.059630464839, \"total_output_tokens\": 2101470.0, \"conso_all_conv\": 9.09769793379999, \"n_match\": 1786, \"total_output_tokens_right\": 2101470.0, \"mean_conso_per_match\": 0.005093895819596859, \"mean_conso_per_token\": 4.329206666666662e-06}, {\"name\": \"Mistral-Small-24B-Instruct-2501\", \"model_name\": \"mistral-small-24b-instruct-2501\", \"organization\": \"Mistral\", \"license\": \"Apache 2.0\", \"elo_score\": 958.3581933285648, \"total_output_tokens\": 2440992.0, \"conso_all_conv\": 14.678287007359955, \"n_match\": 1094, \"total_output_tokens_right\": 2440992.0, \"mean_conso_per_match\": 0.013417081359561203, \"mean_conso_per_token\": 6.013246666666649e-06}, {\"name\": \"Mistral Small-3.1 24B\", \"model_name\": \"mistral-small-3.1-24b\", \"organization\": \"Mistral AI\", \"license\": \"Apache 2.0\", \"elo_score\": 1024.3801866853216, \"total_output_tokens\": 841162.0, \"conso_all_conv\": 5.058114592626664, \"n_match\": 513, \"total_output_tokens_right\": 841162.0, \"mean_conso_per_match\": 0.009859872500246909, \"mean_conso_per_token\": 6.013246666666663e-06}, {\"name\": \"Mixtral 8x22B Instruct\", \"model_name\": \"mixtral-8x22b-instruct-v0.1\", \"organization\": \"Mistral AI\", \"license\": \"Apache 2.0\", \"elo_score\": 886.9488960122875, \"total_output_tokens\": 2889185.0, \"conso_all_conv\": 50.96510783260014, \"n_match\": 1756, \"total_output_tokens_right\": 2889185.0, \"mean_conso_per_match\": 0.029023409927448826, \"mean_conso_per_token\": 1.763996000000005e-05}, {\"name\": \"Mixtral 8x7B Instruct\", \"model_name\": \"mixtral-8x7b-instruct-v0.1\", \"organization\": \"Mistral AI\", \"license\": \"Apache 2.0\", \"elo_score\": 1000.331257052885, \"total_output_tokens\": 900068.0, \"conso_all_conv\": 4.14920547184, \"n_match\": 675, \"total_output_tokens_right\": 900068.0, \"mean_conso_per_match\": 0.006146971069392593, \"mean_conso_per_token\": 4.60988e-06}, {\"name\": \"o3-mini\", \"model_name\": \"o3-mini\", \"organization\": \"OpenAI\", \"license\": \"Proprietary\", \"elo_score\": 941.8837486762151, \"total_output_tokens\": 928508.0, \"conso_all_conv\": 57.0336039000001, \"n_match\": 437, \"total_output_tokens_right\": 928508.0, \"mean_conso_per_match\": 0.13051167940503455, \"mean_conso_per_token\": 6.142500000000011e-05}, {\"name\": \"Phi-3.5 Mini Instruct\", \"model_name\": \"phi-3.5-mini-instruct\", \"organization\": \"Microsoft\", \"license\": \"MIT\", \"elo_score\": 859.9439075250559, \"total_output_tokens\": 851005.0, \"conso_all_conv\": 2.609331674216668, \"n_match\": 462, \"total_output_tokens_right\": 851005.0, \"mean_conso_per_match\": 0.0056479040567460346, \"mean_conso_per_token\": 3.066176666666668e-06}, {\"name\": \"Phi 4\", \"model_name\": \"phi-4\", \"organization\": \"Microsoft\", \"license\": \"MIT\", \"elo_score\": 907.8272991447487, \"total_output_tokens\": 3086417.0, \"conso_all_conv\": 14.228011999959978, \"n_match\": 1616, \"total_output_tokens_right\": 3086417.0, \"mean_conso_per_match\": 0.008804462871262363, \"mean_conso_per_token\": 4.609879999999993e-06}, {\"name\": \"Qwen2.5-7B\", \"model_name\": \"qwen2.5-7b-instruct\", \"organization\": \"Alibaba\", \"license\": \"Apache 2.0\", \"elo_score\": 932.1394504525656, \"total_output_tokens\": 870902.0, \"conso_all_conv\": 3.159217326046669, \"n_match\": 427, \"total_output_tokens_right\": 870902.0, \"mean_conso_per_match\": 0.007398635423996883, \"mean_conso_per_token\": 3.627523333333336e-06}, {\"name\": \"Qwen2.5-Coder-32B-Instruct\", \"model_name\": \"qwen2.5-coder-32b-instruct\", \"organization\": \"Alibaba\", \"license\": \"Apache 2.0\", \"elo_score\": 942.2499395325128, \"total_output_tokens\": 4787749.0, \"conso_all_conv\": 34.16508959906006, \"n_match\": 1641, \"total_output_tokens_right\": 4787749.0, \"mean_conso_per_match\": 0.02081967678187694, \"mean_conso_per_token\": 7.135940000000013e-06}, {\"name\": \"QwQ 32B\", \"model_name\": \"qwq-32b\", \"organization\": \"Alibaba\", \"license\": \"Apache 2.0\", \"elo_score\": 1024.5967895935885, \"total_output_tokens\": 946986.0, \"conso_all_conv\": 6.757635276840006, \"n_match\": 321, \"total_output_tokens_right\": 946986.0, \"mean_conso_per_match\": 0.021051823292336465, \"mean_conso_per_token\": 7.135940000000006e-06}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_chart(final_df, title=\"consommation selon classement\", log=True, scale=\"token\", mean=False, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
