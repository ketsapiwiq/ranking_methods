{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "cache_dir = input(\"Indicate path to all Hugging Face caches:\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = cache_dir\n",
    "os.environ[\"HF_HUB_CACHE\"] = cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since ministere-culture/comparia-reactions couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'default' at /home/jupyterhub-users/shared/projet_comparia/huggingface_hub/ministere-culture___comparia-reactions/default/0.0.0/5a02c58f54f2db3fc51f076d24a840f04efa01fa (last modified on Tue Sep 30 13:43:20 2025).\n",
      "Using the latest cached version of the dataset since ministere-culture/comparia-conversations couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'default' at /home/jupyterhub-users/shared/projet_comparia/huggingface_hub/ministere-culture___comparia-conversations/default/0.0.0/dc40af6af1c14e68bf39d55f6e1573d2d6582f19 (last modified on Wed Jun  4 17:40:30 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from rank_comparia.utils import load_comparia\n",
    "\n",
    "token = None\n",
    "\n",
    "reactions = load_comparia(\"ministere-culture/comparia-reactions\", token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul des scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule des scores comme dans le notebook `rankers.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_comparia.data_transformation import get_matches_with_score\n",
    "\n",
    "matches = get_matches_with_score(reactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_a_name</th><th>model_b_name</th><th>conversation_pair_id</th><th>score_a</th><th>score_b</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;gemma-2-9b-it&quot;</td><td>&quot;phi-4&quot;</td><td>&quot;0dcca21a75b34e0c955dcda5610e7a…</td><td>0</td><td>2</td></tr><tr><td>&quot;deepseek-r1&quot;</td><td>&quot;lfm-40b&quot;</td><td>&quot;7fd245aad4884b7692a433ed5b567f…</td><td>1</td><td>0</td></tr><tr><td>&quot;lfm-40b&quot;</td><td>&quot;phi-4&quot;</td><td>&quot;45d9735b126f4bf2a56133757f7890…</td><td>-1</td><td>2</td></tr><tr><td>&quot;llama-3.1-8b&quot;</td><td>&quot;llama-3.1-70b&quot;</td><td>&quot;516432a3e85448f98264e44ce4e102…</td><td>2</td><td>0</td></tr><tr><td>&quot;llama-3.1-405b&quot;</td><td>&quot;gpt-4o-2024-08-06&quot;</td><td>&quot;56106cc5d1c24061ae5be389f832e9…</td><td>2</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌────────────────┬───────────────────┬─────────────────────────────────┬─────────┬─────────┐\n",
       "│ model_a_name   ┆ model_b_name      ┆ conversation_pair_id            ┆ score_a ┆ score_b │\n",
       "│ ---            ┆ ---               ┆ ---                             ┆ ---     ┆ ---     │\n",
       "│ str            ┆ str               ┆ str                             ┆ i64     ┆ i64     │\n",
       "╞════════════════╪═══════════════════╪═════════════════════════════════╪═════════╪═════════╡\n",
       "│ gemma-2-9b-it  ┆ phi-4             ┆ 0dcca21a75b34e0c955dcda5610e7a… ┆ 0       ┆ 2       │\n",
       "│ deepseek-r1    ┆ lfm-40b           ┆ 7fd245aad4884b7692a433ed5b567f… ┆ 1       ┆ 0       │\n",
       "│ lfm-40b        ┆ phi-4             ┆ 45d9735b126f4bf2a56133757f7890… ┆ -1      ┆ 2       │\n",
       "│ llama-3.1-8b   ┆ llama-3.1-70b     ┆ 516432a3e85448f98264e44ce4e102… ┆ 2       ┆ 0       │\n",
       "│ llama-3.1-405b ┆ gpt-4o-2024-08-06 ┆ 56106cc5d1c24061ae5be389f832e9… ┆ 2       ┆ 0       │\n",
       "└────────────────┴───────────────────┴─────────────────────────────────┴─────────┴─────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_comparia.elo import ELORanker\n",
    "from rank_comparia.ranker import Match, MatchScore\n",
    "import random\n",
    "\n",
    "\n",
    "def compute_match_score(score_a: int, score_b: int) -> MatchScore:\n",
    "    final_score = score_b - score_a\n",
    "    if final_score > 0:\n",
    "        return MatchScore.B\n",
    "    elif final_score < 0:\n",
    "        return MatchScore.A\n",
    "    else:\n",
    "        return MatchScore.Draw\n",
    "\n",
    "\n",
    "def get_shuffled_results(matches: list[Match], model_names: list[str], seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    ranker_shuffle = ELORanker(K=40)\n",
    "    matches_shuffle = random.sample(matches, k=len(matches))\n",
    "    ranker_shuffle.add_players(model_names)\n",
    "    ranker_shuffle.compute_scores(matches=matches_shuffle)\n",
    "    return ranker_shuffle.players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = set(matches[\"model_a_name\"].unique()) | set(matches[\"model_b_name\"].unique())\n",
    "matches = [\n",
    "    Match(\n",
    "        match_dict[\"model_a_name\"],\n",
    "        match_dict[\"model_b_name\"],\n",
    "        compute_match_score(match_dict[\"score_a\"], match_dict[\"score_b\"]),\n",
    "    )\n",
    "    for match_dict in matches.to_dicts()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gemini-2.0-flash-001': 1191.5329357657135,\n",
       " 'gemini-2.0-flash-exp': 1185.4062152435838,\n",
       " 'llama-3.1-nemotron-70b-instruct': 1163.287438813354,\n",
       " 'claude-3-7-sonnet': 1147.664863168684,\n",
       " 'gemma-3-12b': 1131.8831481484765,\n",
       " 'gemma-3-27b': 1117.443728464529,\n",
       " 'grok-3-mini-beta': 1116.3770843103694,\n",
       " 'deepseek-v3-chat': 1107.8446355297663,\n",
       " 'gpt-4.1-mini': 1102.8376391369948,\n",
       " 'o3-mini': 1099.9225486428043,\n",
       " 'deepseek-v3-0324': 1096.7979069789174,\n",
       " 'command-a': 1076.6993409767708,\n",
       " 'llama-3.3-70b': 1068.1432663577077,\n",
       " 'gpt-4o-2024-08-06': 1057.096669419037,\n",
       " 'llama-3.1-405b': 1053.8368983238377,\n",
       " 'jamba-1.5-large': 1052.3335067124294,\n",
       " 'gpt-4o-mini-2024-07-18': 1045.753746620167,\n",
       " 'deepseek-r1': 1038.132216945894,\n",
       " 'mistral-small-3.1-24b': 1031.3332867794068,\n",
       " 'gemini-1.5-pro-001': 1028.8062038307364,\n",
       " 'c4ai-command-r-08-2024': 1026.1078459872717,\n",
       " 'deepseek-r1-distill-llama-70b': 1023.9009626339865,\n",
       " 'mistral-large-2411': 1021.1818119517577,\n",
       " 'mistral-small-24b-instruct-2501': 1020.1841211239736,\n",
       " 'o4-mini': 1017.7682469957736,\n",
       " 'ministral-8b-instruct-2410': 1009.4973292190366,\n",
       " 'llama-4-scout': 1004.5986289379314,\n",
       " 'gemma-3-4b': 1000.0696471316659,\n",
       " 'hermes-3-llama-3.1-405b': 997.5119899101867,\n",
       " 'llama-3.1-70b': 984.2237072856794,\n",
       " 'gemma-2-27b-it-q8': 978.7218050990575,\n",
       " 'gpt-4.1-nano': 974.7073536789148,\n",
       " 'mistral-saba': 959.1412954729836,\n",
       " 'phi-4': 957.6144707146054,\n",
       " 'qwen2.5-coder-32b-instruct': 952.9819928502499,\n",
       " 'qwen2.5-7b-instruct': 952.5034448701352,\n",
       " 'gemma-2-9b-it': 932.0183994764312,\n",
       " 'qwq-32b': 915.7228106812046,\n",
       " 'aya-expanse-8b': 914.2657279721368,\n",
       " 'gemini-1.5-pro-002': 878.6459642748503,\n",
       " 'claude-3-5-sonnet-v2': 877.4451708022083,\n",
       " 'mixtral-8x22b-instruct-v0.1': 846.785454718372,\n",
       " 'llama-3.1-8b': 840.6740311307111,\n",
       " 'chocolatine-2-14b-instruct-v2.0.3-q8': 817.8549526150206,\n",
       " 'mixtral-8x7b-instruct-v0.1': 814.8087648946796,\n",
       " 'mistral-nemo-2407': 804.604573329981,\n",
       " 'phi-3.5-mini-instruct': 797.5700958468177,\n",
       " 'lfm-40b': 765.7561202251927}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker = ELORanker(K=40)\n",
    "\n",
    "random.seed(1337)\n",
    "matches = random.sample(matches, k=len(matches))\n",
    "ranker.add_players(model_names)  # type: ignore\n",
    "ranker.compute_scores(matches=matches)\n",
    "ranker.get_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul d'un score de frugalité\n",
    "\n",
    "Le score de frugalité est calculé à partir de données de consommation présentes dans le jeu de données `comparia-conversations`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul du nombre de match et du nombre total de tokens générés par modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (48, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_name</th><th>n_match</th><th>total_output_tokens</th></tr><tr><td>str</td><td>u32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;aya-expanse-8b&quot;</td><td>965</td><td>961445.0</td></tr><tr><td>&quot;c4ai-command-r-08-2024&quot;</td><td>2571</td><td>5.944341e6</td></tr><tr><td>&quot;chocolatine-2-14b-instruct-v2.…</td><td>1098</td><td>511086.0</td></tr><tr><td>&quot;claude-3-5-sonnet-v2&quot;</td><td>3537</td><td>2.818225e6</td></tr><tr><td>&quot;claude-3-7-sonnet&quot;</td><td>557</td><td>2.162331e6</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;phi-3.5-mini-instruct&quot;</td><td>786</td><td>851005.0</td></tr><tr><td>&quot;phi-4&quot;</td><td>2927</td><td>3.086417e6</td></tr><tr><td>&quot;qwen2.5-7b-instruct&quot;</td><td>756</td><td>870902.0</td></tr><tr><td>&quot;qwen2.5-coder-32b-instruct&quot;</td><td>3222</td><td>4.787749e6</td></tr><tr><td>&quot;qwq-32b&quot;</td><td>577</td><td>946986.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (48, 3)\n",
       "┌─────────────────────────────────┬─────────┬─────────────────────┐\n",
       "│ model_name                      ┆ n_match ┆ total_output_tokens │\n",
       "│ ---                             ┆ ---     ┆ ---                 │\n",
       "│ str                             ┆ u32     ┆ f64                 │\n",
       "╞═════════════════════════════════╪═════════╪═════════════════════╡\n",
       "│ aya-expanse-8b                  ┆ 965     ┆ 961445.0            │\n",
       "│ c4ai-command-r-08-2024          ┆ 2571    ┆ 5.944341e6          │\n",
       "│ chocolatine-2-14b-instruct-v2.… ┆ 1098    ┆ 511086.0            │\n",
       "│ claude-3-5-sonnet-v2            ┆ 3537    ┆ 2.818225e6          │\n",
       "│ claude-3-7-sonnet               ┆ 557     ┆ 2.162331e6          │\n",
       "│ …                               ┆ …       ┆ …                   │\n",
       "│ phi-3.5-mini-instruct           ┆ 786     ┆ 851005.0            │\n",
       "│ phi-4                           ┆ 2927    ┆ 3.086417e6          │\n",
       "│ qwen2.5-7b-instruct             ┆ 756     ┆ 870902.0            │\n",
       "│ qwen2.5-coder-32b-instruct      ┆ 3222    ┆ 4.787749e6          │\n",
       "│ qwq-32b                         ┆ 577     ┆ 946986.0            │\n",
       "└─────────────────────────────────┴─────────┴─────────────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "from rank_comparia.frugality import get_n_match, get_models_output_tokens\n",
    "\n",
    "reactions = reactions.rename({\"model_a_name\": \"model_a\", \"model_b_name\": \"model_b\"})\n",
    "number_by_model = get_n_match(reactions)\n",
    "total_tokens = get_models_output_tokens(reactions)\n",
    "\n",
    "number_by_model = number_by_model.join(total_tokens, on=\"model_name\")\n",
    "\n",
    "number_by_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul du score de frugalité\n",
    "\n",
    "Calcul du score énergétique. Il est possible de moyenner les scores avec le paramètre `mean` (si True, le score est moyenné, sinon non).  \n",
    "Si on décide de moyenner, le moyennage par tokens et par nombre de match est effectué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (48, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_name</th><th>total_output_tokens</th><th>conso_all_conv</th><th>n_match</th><th>total_output_tokens_right</th><th>mean_conso_per_match</th><th>mean_conso_per_token</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;aya-expanse-8b&quot;</td><td>961445.0</td><td>3.62259</td><td>965</td><td>961445.0</td><td>0.003754</td><td>0.000004</td></tr><tr><td>&quot;c4ai-command-r-08-2024&quot;</td><td>5.944341e6</td><td>44.921088</td><td>2571</td><td>5.944341e6</td><td>0.017472</td><td>0.000008</td></tr><tr><td>&quot;chocolatine-2-14b-instruct-v2.…</td><td>511086.0</td><td>1.853976</td><td>1098</td><td>511086.0</td><td>0.001689</td><td>0.000004</td></tr><tr><td>&quot;claude-3-5-sonnet-v2&quot;</td><td>2.818225e6</td><td>378.314297</td><td>3537</td><td>2.818225e6</td><td>0.106959</td><td>0.000134</td></tr><tr><td>&quot;claude-3-7-sonnet&quot;</td><td>2.162331e6</td><td>290.26807</td><td>557</td><td>2.162331e6</td><td>0.521128</td><td>0.000134</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;phi-3.5-mini-instruct&quot;</td><td>851005.0</td><td>2.609332</td><td>786</td><td>851005.0</td><td>0.00332</td><td>0.000003</td></tr><tr><td>&quot;phi-4&quot;</td><td>3.086417e6</td><td>14.228012</td><td>2927</td><td>3.086417e6</td><td>0.004861</td><td>0.000005</td></tr><tr><td>&quot;qwen2.5-7b-instruct&quot;</td><td>870902.0</td><td>3.159217</td><td>756</td><td>870902.0</td><td>0.004179</td><td>0.000004</td></tr><tr><td>&quot;qwen2.5-coder-32b-instruct&quot;</td><td>4.787749e6</td><td>34.16509</td><td>3222</td><td>4.787749e6</td><td>0.010604</td><td>0.000007</td></tr><tr><td>&quot;qwq-32b&quot;</td><td>946986.0</td><td>6.757635</td><td>577</td><td>946986.0</td><td>0.011712</td><td>0.000007</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (48, 7)\n",
       "┌──────────────┬──────────────┬──────────────┬─────────┬──────────────┬──────────────┬─────────────┐\n",
       "│ model_name   ┆ total_output ┆ conso_all_co ┆ n_match ┆ total_output ┆ mean_conso_p ┆ mean_conso_ │\n",
       "│ ---          ┆ _tokens      ┆ nv           ┆ ---     ┆ _tokens_righ ┆ er_match     ┆ per_token   │\n",
       "│ str          ┆ ---          ┆ ---          ┆ u32     ┆ t            ┆ ---          ┆ ---         │\n",
       "│              ┆ f64          ┆ f64          ┆         ┆ ---          ┆ f64          ┆ f64         │\n",
       "│              ┆              ┆              ┆         ┆ f64          ┆              ┆             │\n",
       "╞══════════════╪══════════════╪══════════════╪═════════╪══════════════╪══════════════╪═════════════╡\n",
       "│ aya-expanse- ┆ 961445.0     ┆ 3.62259      ┆ 965     ┆ 961445.0     ┆ 0.003754     ┆ 0.000004    │\n",
       "│ 8b           ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ c4ai-command ┆ 5.944341e6   ┆ 44.921088    ┆ 2571    ┆ 5.944341e6   ┆ 0.017472     ┆ 0.000008    │\n",
       "│ -r-08-2024   ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ chocolatine- ┆ 511086.0     ┆ 1.853976     ┆ 1098    ┆ 511086.0     ┆ 0.001689     ┆ 0.000004    │\n",
       "│ 2-14b-instru ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ ct-v2.…      ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ claude-3-5-s ┆ 2.818225e6   ┆ 378.314297   ┆ 3537    ┆ 2.818225e6   ┆ 0.106959     ┆ 0.000134    │\n",
       "│ onnet-v2     ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ claude-3-7-s ┆ 2.162331e6   ┆ 290.26807    ┆ 557     ┆ 2.162331e6   ┆ 0.521128     ┆ 0.000134    │\n",
       "│ onnet        ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ …            ┆ …            ┆ …            ┆ …       ┆ …            ┆ …            ┆ …           │\n",
       "│ phi-3.5-mini ┆ 851005.0     ┆ 2.609332     ┆ 786     ┆ 851005.0     ┆ 0.00332      ┆ 0.000003    │\n",
       "│ -instruct    ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ phi-4        ┆ 3.086417e6   ┆ 14.228012    ┆ 2927    ┆ 3.086417e6   ┆ 0.004861     ┆ 0.000005    │\n",
       "│ qwen2.5-7b-i ┆ 870902.0     ┆ 3.159217     ┆ 756     ┆ 870902.0     ┆ 0.004179     ┆ 0.000004    │\n",
       "│ nstruct      ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ qwen2.5-code ┆ 4.787749e6   ┆ 34.16509     ┆ 3222    ┆ 4.787749e6   ┆ 0.010604     ┆ 0.000007    │\n",
       "│ r-32b-instru ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ ct           ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ qwq-32b      ┆ 946986.0     ┆ 6.757635     ┆ 577     ┆ 946986.0     ┆ 0.011712     ┆ 0.000007    │\n",
       "└──────────────┴──────────────┴──────────────┴─────────┴──────────────┴──────────────┴─────────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rank_comparia.frugality import calculate_frugality_score\n",
    "\n",
    "frugal_scores = calculate_frugality_score(reactions, number_by_model)\n",
    "\n",
    "frugal_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (48, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_name</th><th>elo_score</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;gemini-2.0-flash-001&quot;</td><td>1191.532936</td></tr><tr><td>&quot;gemini-2.0-flash-exp&quot;</td><td>1185.406215</td></tr><tr><td>&quot;llama-3.1-nemotron-70b-instruc…</td><td>1163.287439</td></tr><tr><td>&quot;claude-3-7-sonnet&quot;</td><td>1147.664863</td></tr><tr><td>&quot;gemma-3-12b&quot;</td><td>1131.883148</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;chocolatine-2-14b-instruct-v2.…</td><td>817.854953</td></tr><tr><td>&quot;mixtral-8x7b-instruct-v0.1&quot;</td><td>814.808765</td></tr><tr><td>&quot;mistral-nemo-2407&quot;</td><td>804.604573</td></tr><tr><td>&quot;phi-3.5-mini-instruct&quot;</td><td>797.570096</td></tr><tr><td>&quot;lfm-40b&quot;</td><td>765.75612</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (48, 2)\n",
       "┌─────────────────────────────────┬─────────────┐\n",
       "│ model_name                      ┆ elo_score   │\n",
       "│ ---                             ┆ ---         │\n",
       "│ str                             ┆ f64         │\n",
       "╞═════════════════════════════════╪═════════════╡\n",
       "│ gemini-2.0-flash-001            ┆ 1191.532936 │\n",
       "│ gemini-2.0-flash-exp            ┆ 1185.406215 │\n",
       "│ llama-3.1-nemotron-70b-instruc… ┆ 1163.287439 │\n",
       "│ claude-3-7-sonnet               ┆ 1147.664863 │\n",
       "│ gemma-3-12b                     ┆ 1131.883148 │\n",
       "│ …                               ┆ …           │\n",
       "│ chocolatine-2-14b-instruct-v2.… ┆ 817.854953  │\n",
       "│ mixtral-8x7b-instruct-v0.1      ┆ 814.808765  │\n",
       "│ mistral-nemo-2407               ┆ 804.604573  │\n",
       "│ phi-3.5-mini-instruct           ┆ 797.570096  │\n",
       "│ lfm-40b                         ┆ 765.75612   │\n",
       "└─────────────────────────────────┴─────────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elo_scores = pl.DataFrame(\n",
    "    {\n",
    "        \"model_name\": ranker.players.keys(),\n",
    "        \"elo_score\": ranker.players.values(),\n",
    "    },\n",
    "    strict=False,\n",
    ").sort(by=\"elo_score\", descending=True)\n",
    "\n",
    "elo_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du graphique de frugalité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des informations concernant les modèles du comparateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "info_model = pl.read_json(source=Path(\".\").resolve().parent / \"data\" / \"models_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (39, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>name</th><th>model_name</th><th>organization</th><th>license</th><th>elo_score</th><th>total_output_tokens</th><th>conso_all_conv</th><th>n_match</th><th>total_output_tokens_right</th><th>mean_conso_per_match</th><th>mean_conso_per_token</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Aya-Expanse-8B&quot;</td><td>&quot;aya-expanse-8b&quot;</td><td>&quot;Cohere&quot;</td><td>&quot;CC-BY-NC-4.0&quot;</td><td>914.265728</td><td>961445.0</td><td>3.62259</td><td>965</td><td>961445.0</td><td>0.003754</td><td>0.000004</td></tr><tr><td>&quot;Command R (08-2024)&quot;</td><td>&quot;c4ai-command-r-08-2024&quot;</td><td>&quot;Cohere&quot;</td><td>&quot;CC-BY-NC-4.0&quot;</td><td>1026.107846</td><td>5.944341e6</td><td>44.921088</td><td>2571</td><td>5.944341e6</td><td>0.017472</td><td>0.000008</td></tr><tr><td>&quot;Chocolatine-2-14b Instruct&quot;</td><td>&quot;chocolatine-2-14b-instruct-v2.…</td><td>&quot;jpacifico (individual)&quot;</td><td>&quot;Apache 2.0&quot;</td><td>817.854953</td><td>511086.0</td><td>1.853976</td><td>1098</td><td>511086.0</td><td>0.001689</td><td>0.000004</td></tr><tr><td>&quot;Claude 3.5 Sonnet V2&quot;</td><td>&quot;claude-3-5-sonnet-v2&quot;</td><td>&quot;Anthropic&quot;</td><td>&quot;Proprietary&quot;</td><td>877.445171</td><td>2.818225e6</td><td>378.314297</td><td>3537</td><td>2.818225e6</td><td>0.106959</td><td>0.000134</td></tr><tr><td>&quot;Command A&quot;</td><td>&quot;command-a&quot;</td><td>&quot;Cohere&quot;</td><td>&quot;CC-BY-NC-4.0&quot;</td><td>1076.699341</td><td>1.03253e6</td><td>18.815316</td><td>760</td><td>1.03253e6</td><td>0.024757</td><td>0.000018</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;Phi-3.5 Mini Instruct&quot;</td><td>&quot;phi-3.5-mini-instruct&quot;</td><td>&quot;Microsoft&quot;</td><td>&quot;MIT&quot;</td><td>797.570096</td><td>851005.0</td><td>2.609332</td><td>786</td><td>851005.0</td><td>0.00332</td><td>0.000003</td></tr><tr><td>&quot;Phi 4&quot;</td><td>&quot;phi-4&quot;</td><td>&quot;Microsoft&quot;</td><td>&quot;MIT&quot;</td><td>957.614471</td><td>3.086417e6</td><td>14.228012</td><td>2927</td><td>3.086417e6</td><td>0.004861</td><td>0.000005</td></tr><tr><td>&quot;Qwen2.5-7B&quot;</td><td>&quot;qwen2.5-7b-instruct&quot;</td><td>&quot;Alibaba&quot;</td><td>&quot;Apache 2.0&quot;</td><td>952.503445</td><td>870902.0</td><td>3.159217</td><td>756</td><td>870902.0</td><td>0.004179</td><td>0.000004</td></tr><tr><td>&quot;Qwen2.5-Coder-32B-Instruct&quot;</td><td>&quot;qwen2.5-coder-32b-instruct&quot;</td><td>&quot;Alibaba&quot;</td><td>&quot;Apache 2.0&quot;</td><td>952.981993</td><td>4.787749e6</td><td>34.16509</td><td>3222</td><td>4.787749e6</td><td>0.010604</td><td>0.000007</td></tr><tr><td>&quot;QwQ 32B&quot;</td><td>&quot;qwq-32b&quot;</td><td>&quot;Alibaba&quot;</td><td>&quot;Apache 2.0&quot;</td><td>915.722811</td><td>946986.0</td><td>6.757635</td><td>577</td><td>946986.0</td><td>0.011712</td><td>0.000007</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (39, 11)\n",
       "┌────────────┬───────────┬───────────┬───────────┬───┬─────────┬───────────┬───────────┬───────────┐\n",
       "│ name       ┆ model_nam ┆ organizat ┆ license   ┆ … ┆ n_match ┆ total_out ┆ mean_cons ┆ mean_cons │\n",
       "│ ---        ┆ e         ┆ ion       ┆ ---       ┆   ┆ ---     ┆ put_token ┆ o_per_mat ┆ o_per_tok │\n",
       "│ str        ┆ ---       ┆ ---       ┆ str       ┆   ┆ u32     ┆ s_right   ┆ ch        ┆ en        │\n",
       "│            ┆ str       ┆ str       ┆           ┆   ┆         ┆ ---       ┆ ---       ┆ ---       │\n",
       "│            ┆           ┆           ┆           ┆   ┆         ┆ f64       ┆ f64       ┆ f64       │\n",
       "╞════════════╪═══════════╪═══════════╪═══════════╪═══╪═════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ Aya-Expans ┆ aya-expan ┆ Cohere    ┆ CC-BY-NC- ┆ … ┆ 965     ┆ 961445.0  ┆ 0.003754  ┆ 0.000004  │\n",
       "│ e-8B       ┆ se-8b     ┆           ┆ 4.0       ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Command R  ┆ c4ai-comm ┆ Cohere    ┆ CC-BY-NC- ┆ … ┆ 2571    ┆ 5.944341e ┆ 0.017472  ┆ 0.000008  │\n",
       "│ (08-2024)  ┆ and-r-08- ┆           ┆ 4.0       ┆   ┆         ┆ 6         ┆           ┆           │\n",
       "│            ┆ 2024      ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Chocolatin ┆ chocolati ┆ jpacifico ┆ Apache    ┆ … ┆ 1098    ┆ 511086.0  ┆ 0.001689  ┆ 0.000004  │\n",
       "│ e-2-14b    ┆ ne-2-14b- ┆ (individu ┆ 2.0       ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Instruct   ┆ instruct- ┆ al)       ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│            ┆ v2.…      ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Claude 3.5 ┆ claude-3- ┆ Anthropic ┆ Proprieta ┆ … ┆ 3537    ┆ 2.818225e ┆ 0.106959  ┆ 0.000134  │\n",
       "│ Sonnet V2  ┆ 5-sonnet- ┆           ┆ ry        ┆   ┆         ┆ 6         ┆           ┆           │\n",
       "│            ┆ v2        ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Command A  ┆ command-a ┆ Cohere    ┆ CC-BY-NC- ┆ … ┆ 760     ┆ 1.03253e6 ┆ 0.024757  ┆ 0.000018  │\n",
       "│            ┆           ┆           ┆ 4.0       ┆   ┆         ┆           ┆           ┆           │\n",
       "│ …          ┆ …         ┆ …         ┆ …         ┆ … ┆ …       ┆ …         ┆ …         ┆ …         │\n",
       "│ Phi-3.5    ┆ phi-3.5-m ┆ Microsoft ┆ MIT       ┆ … ┆ 786     ┆ 851005.0  ┆ 0.00332   ┆ 0.000003  │\n",
       "│ Mini       ┆ ini-instr ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Instruct   ┆ uct       ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Phi 4      ┆ phi-4     ┆ Microsoft ┆ MIT       ┆ … ┆ 2927    ┆ 3.086417e ┆ 0.004861  ┆ 0.000005  │\n",
       "│            ┆           ┆           ┆           ┆   ┆         ┆ 6         ┆           ┆           │\n",
       "│ Qwen2.5-7B ┆ qwen2.5-7 ┆ Alibaba   ┆ Apache    ┆ … ┆ 756     ┆ 870902.0  ┆ 0.004179  ┆ 0.000004  │\n",
       "│            ┆ b-instruc ┆           ┆ 2.0       ┆   ┆         ┆           ┆           ┆           │\n",
       "│            ┆ t         ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Qwen2.5-Co ┆ qwen2.5-c ┆ Alibaba   ┆ Apache    ┆ … ┆ 3222    ┆ 4.787749e ┆ 0.010604  ┆ 0.000007  │\n",
       "│ der-32B-In ┆ oder-32b- ┆           ┆ 2.0       ┆   ┆         ┆ 6         ┆           ┆           │\n",
       "│ struct     ┆ instruct  ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ QwQ 32B    ┆ qwq-32b   ┆ Alibaba   ┆ Apache    ┆ … ┆ 577     ┆ 946986.0  ┆ 0.011712  ┆ 0.000007  │\n",
       "│            ┆           ┆           ┆ 2.0       ┆   ┆         ┆           ┆           ┆           │\n",
       "└────────────┴───────────┴───────────┴───────────┴───┴─────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = info_model.join(elo_scores, on=\"model_name\").join(frugal_scores, on=\"model_name\")\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_comparia.utils import save_data\n",
    "\n",
    "save_path = Path(\".\").resolve().parent / \"data\"\n",
    "save_data(final_df, \"all_info_for_chart_drawing\", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération du graphique de frugalité\n",
    "\n",
    "Les paramètres possibles :  \n",
    "- `log` : Ajuster l'échelle du graphique en linéaire (`log = False`) ou en log (`log = True`) ; \n",
    "- `mean` : Utiliser les consommation moyenné (`mean = True`) ou non (`mean = False`) ;  \n",
    "- `scale` :  choix du moyennage si `mean = True`. `token` si on utilise le moyennage par token, `match` si on utilise le moyennage par nombre de match ;  \n",
    "- `save` : Enregistrement du graphique au format html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-ed146ef9b08d44b9b417d2cf8c133955.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-ed146ef9b08d44b9b417d2cf8c133955.vega-embed details,\n",
       "  #altair-viz-ed146ef9b08d44b9b417d2cf8c133955.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-ed146ef9b08d44b9b417d2cf8c133955\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-ed146ef9b08d44b9b417d2cf8c133955\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-ed146ef9b08d44b9b417d2cf8c133955\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-f43160427980a23ef455a57cab89ae75\"}, \"mark\": {\"type\": \"point\", \"filled\": true}, \"encoding\": {\"color\": {\"field\": \"organization\", \"type\": \"nominal\"}, \"opacity\": {\"condition\": [{\"param\": \"param_2\", \"value\": 1}], \"value\": 0.3}, \"tooltip\": [{\"field\": \"model_name\", \"type\": \"nominal\"}, {\"field\": \"organization\", \"type\": \"nominal\"}, {\"field\": \"license\", \"type\": \"nominal\"}, {\"field\": \"median\", \"type\": \"quantitative\"}, {\"field\": \"mean_conso_per_token\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"mean_conso_per_token\", \"scale\": {\"type\": \"log\"}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"median\", \"scale\": {\"zero\": false}, \"title\": \"elo score\", \"type\": \"quantitative\"}}, \"height\": 300, \"params\": [{\"name\": \"param_2\", \"select\": {\"type\": \"point\", \"fields\": [\"organization\"]}, \"bind\": \"legend\"}, {\"name\": \"param_1\", \"select\": {\"type\": \"point\", \"fields\": [\"license\"]}, \"bind\": {\"input\": \"select\", \"options\": [\"Unknown\", \"CC-BY-NC-4.0\", \"Gemma license\", \"MRL\", \"Mistral Research\", \"Apache 2.0\", \"DeepSeek\", \"MIT\", \"llama3\", \"Jamba Open Model License\", \"Proprietary\", \"Llama 3.1 Community\", \"Llama 3.3 Community\", \"Gemma\"], \"labels\": [\"MRL\", \"Unknown\", \"llama3\", \"Llama 3.1 Community\", \"Gemma\", \"DeepSeek\", \"Jamba Open Model License\", \"CC-BY-NC-4.0\", \"Gemma license\", \"Mistral Research\", \"Apache 2.0\", \"Llama 3.3 Community\", \"MIT\", \"Proprietary\"], \"name\": \"License : \"}}], \"title\": \"consommation selon classement\", \"transform\": [{\"filter\": {\"param\": \"param_1\"}}], \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-f43160427980a23ef455a57cab89ae75\": [{\"name\": \"Claude 3.5 Sonnet V2\", \"model_name\": \"claude-3-5-sonnet-v2\", \"organization\": \"Anthropic\", \"license\": \"Proprietary\", \"median\": 877.4451708022083, \"total_output_tokens\": 2818225.0, \"conso_all_conv\": 378.31429666249966, \"n_match\": 3537, \"total_output_tokens_right\": 2818225.0, \"mean_conso_per_match\": 0.10695908868037876, \"mean_conso_per_token\": 0.00013423849999999988, \"name_right\": \"Claude 3.5 Sonnet V2\", \"organization_right\": \"Anthropic\", \"license_right\": \"Proprietary\"}, {\"name\": \"GPT-4o mini\", \"model_name\": \"gpt-4o-mini-2024-07-18\", \"organization\": \"OpenAI\", \"license\": \"Proprietary\", \"median\": 1045.753746620167, \"total_output_tokens\": 3805215.0, \"conso_all_conv\": 28.75581949424996, \"n_match\": 3857, \"total_output_tokens_right\": 3805215.0, \"mean_conso_per_match\": 0.007455488590679273, \"mean_conso_per_token\": 7.5569499999999895e-06, \"name_right\": \"GPT-4o mini\", \"organization_right\": \"OpenAI\", \"license_right\": \"Proprietary\"}, {\"name\": \"Gemini 2.0 Flash\", \"model_name\": \"gemini-2.0-flash-exp\", \"organization\": \"Google\", \"license\": \"Proprietary\", \"median\": 1185.4062152435838, \"total_output_tokens\": 3076613.0, \"conso_all_conv\": 25.408618675566665, \"n_match\": 1740, \"total_output_tokens_right\": 3076613.0, \"mean_conso_per_match\": 0.01460265441124521, \"mean_conso_per_token\": 8.258633333333333e-06, \"name_right\": \"Gemini 2.0 Flash\", \"organization_right\": \"Google\", \"license_right\": \"Proprietary\"}, {\"name\": \"Llama 3.1 405B\", \"model_name\": \"llama-3.1-405b\", \"organization\": \"Meta\", \"license\": \"Llama 3.1 Community\", \"median\": 1053.8368983238377, \"total_output_tokens\": 4276508.0, \"conso_all_conv\": 1017.4927275085383, \"n_match\": 3793, \"total_output_tokens_right\": 4276508.0, \"mean_conso_per_match\": 0.2682553987631264, \"mean_conso_per_token\": 0.00023792606666666783, \"name_right\": \"Llama 3.1 405B\", \"organization_right\": \"Meta\", \"license_right\": \"Llama 3.1 Community\"}, {\"name\": \"Mistral-Large-2411\", \"model_name\": \"mistral-large-2411\", \"organization\": \"Mistral\", \"license\": \"Mistral Research\", \"median\": 1021.1818119517577, \"total_output_tokens\": 9064145.0, \"conso_all_conv\": 180.43609736028264, \"n_match\": 3300, \"total_output_tokens_right\": 9064145.0, \"mean_conso_per_match\": 0.05467760526069171, \"mean_conso_per_token\": 1.990657666666659e-05, \"name_right\": \"Mistral-Large-2411\", \"organization_right\": \"Mistral\", \"license_right\": \"Mistral Research\"}, {\"name\": \"Llama-3.1-70B\", \"model_name\": \"llama-3.1-70b\", \"organization\": \"Meta\", \"license\": \"Llama 3.1 Community\", \"median\": 984.2237072856794, \"total_output_tokens\": 2450777.0, \"conso_all_conv\": 30.55808487246673, \"n_match\": 2794, \"total_output_tokens_right\": 2450777.0, \"mean_conso_per_match\": 0.010937038250703912, \"mean_conso_per_token\": 1.2468733333333359e-05, \"name_right\": \"Llama-3.1-70B\", \"organization_right\": \"Meta\", \"license_right\": \"Llama 3.1 Community\"}, {\"name\": \"Llama 3.3 70B\", \"model_name\": \"llama-3.3-70b\", \"organization\": \"Meta\", \"license\": \"Llama 3.3 Community\", \"median\": 1068.1432663577077, \"total_output_tokens\": 5793973.0, \"conso_all_conv\": 72.24350427753315, \"n_match\": 2161, \"total_output_tokens_right\": 5793973.0, \"mean_conso_per_match\": 0.03343058967030687, \"mean_conso_per_token\": 1.2468733333333301e-05, \"name_right\": \"Llama 3.3 70B\", \"organization_right\": \"Meta\", \"license_right\": \"Llama 3.3 Community\"}, {\"name\": \"Qwen2.5-Coder-32B-Instruct\", \"model_name\": \"qwen2.5-coder-32b-instruct\", \"organization\": \"Alibaba\", \"license\": \"Apache 2.0\", \"median\": 952.9819928502499, \"total_output_tokens\": 4787749.0, \"conso_all_conv\": 34.16508959906006, \"n_match\": 3222, \"total_output_tokens_right\": 4787749.0, \"mean_conso_per_match\": 0.010603690130062092, \"mean_conso_per_token\": 7.135940000000013e-06, \"name_right\": \"Qwen2.5-Coder-32B-Instruct\", \"organization_right\": \"Alibaba\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Gemini 1.5 pro 001\", \"model_name\": \"gemini-1.5-pro-001\", \"organization\": \"Google\", \"license\": \"Proprietary\", \"median\": 1028.8062038307364, \"total_output_tokens\": 612722.0, \"conso_all_conv\": 82.15188674586669, \"n_match\": 692, \"total_output_tokens_right\": 612722.0, \"mean_conso_per_match\": 0.11871659934373799, \"mean_conso_per_token\": 0.00013407693333333337, \"name_right\": \"Gemini 1.5 pro 001\", \"organization_right\": \"Google\", \"license_right\": \"Proprietary\"}, {\"name\": \"Gemini 1.5 pro 002\", \"model_name\": \"gemini-1.5-pro-002\", \"organization\": \"Google\", \"license\": \"Proprietary\", \"median\": 878.6459642748503, \"total_output_tokens\": 3284077.0, \"conso_all_conv\": 440.3189729905331, \"n_match\": 3021, \"total_output_tokens_right\": 3284077.0, \"mean_conso_per_match\": 0.14575272194324168, \"mean_conso_per_token\": 0.00013407693333333326, \"name_right\": \"Gemini 1.5 pro 002\", \"organization_right\": \"Google\", \"license_right\": \"Proprietary\"}, {\"name\": \"Aya-Expanse-8B\", \"model_name\": \"aya-expanse-8b\", \"organization\": \"Cohere\", \"license\": \"CC-BY-NC-4.0\", \"median\": 914.2657279721368, \"total_output_tokens\": 961445.0, \"conso_all_conv\": 3.6225901576999946, \"n_match\": 965, \"total_output_tokens_right\": 961445.0, \"mean_conso_per_match\": 0.0037539794380310825, \"mean_conso_per_token\": 3.767859999999994e-06, \"name_right\": \"Aya-Expanse-8B\", \"organization_right\": \"Cohere\", \"license_right\": \"CC-BY-NC-4.0\"}, {\"name\": \"Command R (08-2024)\", \"model_name\": \"c4ai-command-r-08-2024\", \"organization\": \"Cohere\", \"license\": \"CC-BY-NC-4.0\", \"median\": 1026.1078459872717, \"total_output_tokens\": 5944341.0, \"conso_all_conv\": 44.92108771994969, \"n_match\": 2571, \"total_output_tokens_right\": 5944341.0, \"mean_conso_per_match\": 0.01747222392841295, \"mean_conso_per_token\": 7.556949999999948e-06, \"name_right\": \"Command R (08-2024)\", \"organization_right\": \"Cohere\", \"license_right\": \"CC-BY-NC-4.0\"}, {\"name\": \"Ministral 8B-Instruct\", \"model_name\": \"ministral-8b-instruct-2410\", \"organization\": \"Mistral\", \"license\": \"MRL\", \"median\": 1009.4973292190366, \"total_output_tokens\": 3227726.0, \"conso_all_conv\": 12.161619686360076, \"n_match\": 3388, \"total_output_tokens_right\": 3227726.0, \"mean_conso_per_match\": 0.0035896162002243435, \"mean_conso_per_token\": 3.767860000000024e-06, \"name_right\": \"Ministral 8B-Instruct\", \"organization_right\": \"Mistral\", \"license_right\": \"MRL\"}, {\"name\": \"Llama 3.1 8B\", \"model_name\": \"llama-3.1-8b\", \"organization\": \"Meta\", \"license\": \"Llama 3.1 Community\", \"median\": 840.6740311307111, \"total_output_tokens\": 2993533.0, \"conso_all_conv\": 11.279213249379985, \"n_match\": 3163, \"total_output_tokens_right\": 2993533.0, \"mean_conso_per_match\": 0.003565985851843182, \"mean_conso_per_token\": 3.767859999999995e-06, \"name_right\": \"Llama 3.1 8B\", \"organization_right\": \"Meta\", \"license_right\": \"Llama 3.1 Community\"}, {\"name\": \"DeepSeek-V3 Chat\", \"model_name\": \"deepseek-v3-chat\", \"organization\": \"DeepSeek\", \"license\": \"DeepSeek\", \"median\": 1107.8446355297663, \"total_output_tokens\": 9475940.0, \"conso_all_conv\": 445.6130906956012, \"n_match\": 3884, \"total_output_tokens_right\": 9475940.0, \"mean_conso_per_match\": 0.11473045589485098, \"mean_conso_per_token\": 4.702574000000013e-05, \"name_right\": \"DeepSeek-V3 Chat\", \"organization_right\": \"DeepSeek\", \"license_right\": \"DeepSeek\"}, {\"name\": \"DeepSeek-R1\", \"model_name\": \"deepseek-r1\", \"organization\": \"DeepSeek\", \"license\": \"MIT\", \"median\": 1038.132216945894, \"total_output_tokens\": 1160060.0, \"conso_all_conv\": 54.55267994439995, \"n_match\": 808, \"total_output_tokens_right\": 1160060.0, \"mean_conso_per_match\": 0.06751569300049498, \"mean_conso_per_token\": 4.7025739999999955e-05, \"name_right\": \"DeepSeek-R1\", \"organization_right\": \"DeepSeek\", \"license_right\": \"MIT\"}, {\"name\": \"DeepSeek-R1 distiil\", \"model_name\": \"deepseek-r1-distill-llama-70b\", \"organization\": \"DeepSeek\", \"license\": \"MIT\", \"median\": 1023.9009626339865, \"total_output_tokens\": 741901.0, \"conso_all_conv\": 9.250565728733326, \"n_match\": 766, \"total_output_tokens_right\": 741901.0, \"mean_conso_per_match\": 0.012076456564926013, \"mean_conso_per_token\": 1.2468733333333323e-05, \"name_right\": \"DeepSeek-R1 distiil\", \"organization_right\": \"DeepSeek\", \"license_right\": \"MIT\"}, {\"name\": \"Gemini 2.0 Flash\", \"model_name\": \"gemini-2.0-flash-001\", \"organization\": \"Google\", \"license\": \"Proprietary\", \"median\": 1191.5329357657135, \"total_output_tokens\": 2300660.0, \"conso_all_conv\": 19.000307364666675, \"n_match\": 1398, \"total_output_tokens_right\": 2300660.0, \"mean_conso_per_match\": 0.01359106392322366, \"mean_conso_per_token\": 8.258633333333338e-06, \"name_right\": \"Gemini 2.0 Flash\", \"organization_right\": \"Google\", \"license_right\": \"Proprietary\"}, {\"name\": \"Gemma 2 9B\", \"model_name\": \"gemma-2-9b-it\", \"organization\": \"Google\", \"license\": \"Gemma license\", \"median\": 932.0183994764312, \"total_output_tokens\": 2016207.0, \"conso_all_conv\": 7.879733476710017, \"n_match\": 2657, \"total_output_tokens_right\": 2016207.0, \"mean_conso_per_match\": 0.002965650536962746, \"mean_conso_per_token\": 3.908196666666675e-06, \"name_right\": \"Gemma 2 9B\", \"organization_right\": \"Google\", \"license_right\": \"Gemma license\"}, {\"name\": \"Gemma 3 12B\", \"model_name\": \"gemma-3-12b\", \"organization\": \"Google\", \"license\": \"Gemma license\", \"median\": 1131.8831481484765, \"total_output_tokens\": 1088635.0, \"conso_all_conv\": 4.712925899566667, \"n_match\": 773, \"total_output_tokens_right\": 1088635.0, \"mean_conso_per_match\": 0.006096928718714963, \"mean_conso_per_token\": 4.329206666666667e-06, \"name_right\": \"Gemma 3 12B\", \"organization_right\": \"Google\", \"license_right\": \"Gemma license\"}, {\"name\": \"Gemma 3 4B\", \"model_name\": \"gemma-3-4b\", \"organization\": \"Google\", \"license\": \"Gemma license\", \"median\": 1000.0696471316659, \"total_output_tokens\": 1147829.0, \"conso_all_conv\": 3.6805289928866673, \"n_match\": 783, \"total_output_tokens_right\": 1147829.0, \"mean_conso_per_match\": 0.00470054788363559, \"mean_conso_per_token\": 3.206513333333334e-06, \"name_right\": \"Gemma 3 4B\", \"organization_right\": \"Google\", \"license_right\": \"Gemma license\"}, {\"name\": \"Gemma 2 27B q8\", \"model_name\": \"gemma-2-27b-it-q8\", \"organization\": \"Google\", \"license\": \"Gemma license\", \"median\": 978.7218050990575, \"total_output_tokens\": 242100.0, \"conso_all_conv\": 1.5577335389999991, \"n_match\": 296, \"total_output_tokens_right\": 242100.0, \"mean_conso_per_match\": 0.0052626133074324295, \"mean_conso_per_token\": 6.434256666666663e-06, \"name_right\": \"Gemma 2 27B q8\", \"organization_right\": \"Google\", \"license_right\": \"Gemma license\"}, {\"name\": \"o3-mini\", \"model_name\": \"o3-mini\", \"organization\": \"OpenAI\", \"license\": \"Proprietary\", \"median\": 1099.9225486428043, \"total_output_tokens\": 928508.0, \"conso_all_conv\": 57.0336039000001, \"n_match\": 743, \"total_output_tokens_right\": 928508.0, \"mean_conso_per_match\": 0.07676124347240929, \"mean_conso_per_token\": 6.142500000000011e-05, \"name_right\": \"o3-mini\", \"organization_right\": \"OpenAI\", \"license_right\": \"Proprietary\"}, {\"name\": \"Mistral-Small-24B-Instruct-2501\", \"model_name\": \"mistral-small-24b-instruct-2501\", \"organization\": \"Mistral\", \"license\": \"Apache 2.0\", \"median\": 1020.1841211239736, \"total_output_tokens\": 2440992.0, \"conso_all_conv\": 14.678287007359955, \"n_match\": 2025, \"total_output_tokens_right\": 2440992.0, \"mean_conso_per_match\": 0.007248536793758003, \"mean_conso_per_token\": 6.013246666666649e-06, \"name_right\": \"Mistral-Small-24B-Instruct-2501\", \"organization_right\": \"Mistral\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Gemma-3 27B\", \"model_name\": \"gemma-3-27b\", \"organization\": \"Google\", \"license\": \"Gemma\", \"median\": 1117.443728464529, \"total_output_tokens\": 1405974.0, \"conso_all_conv\": 9.046397582660008, \"n_match\": 966, \"total_output_tokens_right\": 1405974.0, \"mean_conso_per_match\": 0.009364800810207047, \"mean_conso_per_token\": 6.434256666666672e-06, \"name_right\": \"Gemma-3 27B\", \"organization_right\": \"Google\", \"license_right\": \"Gemma\"}, {\"name\": \"Qwen2.5-7B\", \"model_name\": \"qwen2.5-7b-instruct\", \"organization\": \"Alibaba\", \"license\": \"Apache 2.0\", \"median\": 952.5034448701352, \"total_output_tokens\": 870902.0, \"conso_all_conv\": 3.159217326046669, \"n_match\": 756, \"total_output_tokens_right\": 870902.0, \"mean_conso_per_match\": 0.0041788588968871285, \"mean_conso_per_token\": 3.627523333333336e-06, \"name_right\": \"Qwen2.5-7B\", \"organization_right\": \"Alibaba\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Chocolatine-2-14b Instruct\", \"model_name\": \"chocolatine-2-14b-instruct-v2.0.3-q8\", \"organization\": \"jpacifico (individual)\", \"license\": \"Apache 2.0\", \"median\": 817.8549526150206, \"total_output_tokens\": 511086.0, \"conso_all_conv\": 1.853976390340002, \"n_match\": 1098, \"total_output_tokens_right\": 511086.0, \"mean_conso_per_match\": 0.0016885030877413496, \"mean_conso_per_token\": 3.627523333333337e-06, \"name_right\": \"Chocolatine-2-14b Instruct\", \"organization_right\": \"jpacifico (individual)\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Command A\", \"model_name\": \"command-a\", \"organization\": \"Cohere\", \"license\": \"CC-BY-NC-4.0\", \"median\": 1076.6993409767708, \"total_output_tokens\": 1032530.0, \"conso_all_conv\": 18.815315784433313, \"n_match\": 760, \"total_output_tokens_right\": 1032530.0, \"mean_conso_per_match\": 0.024756994453201727, \"mean_conso_per_token\": 1.8222536666666648e-05, \"name_right\": \"Command A\", \"organization_right\": \"Cohere\", \"license_right\": \"CC-BY-NC-4.0\"}, {\"name\": \"Hermes 3\", \"model_name\": \"hermes-3-llama-3.1-405b\", \"organization\": \"Nous Research\", \"license\": \"llama3\", \"median\": 997.5119899101867, \"total_output_tokens\": 4182418.0, \"conso_all_conv\": 995.1062638958654, \"n_match\": 2756, \"total_output_tokens_right\": 4182418.0, \"mean_conso_per_match\": 0.36106903624668557, \"mean_conso_per_token\": 0.00023792606666666637, \"name_right\": \"Hermes 3\", \"organization_right\": \"Nous Research\", \"license_right\": \"llama3\"}, {\"name\": \"Jamba Large\", \"model_name\": \"jamba-1.5-large\", \"organization\": \"AI21 Labs\", \"license\": \"Jamba Open Model License\", \"median\": 1052.3335067124294, \"total_output_tokens\": 581721.0, \"conso_all_conv\": 27.637820667239993, \"n_match\": 237, \"total_output_tokens_right\": 581721.0, \"mean_conso_per_match\": 0.116615277076962, \"mean_conso_per_token\": 4.751043999999999e-05, \"name_right\": \"Jamba Large\", \"organization_right\": \"AI21 Labs\", \"license_right\": \"Jamba Open Model License\"}, {\"name\": \"Liquid Foundation Model\", \"model_name\": \"lfm-40b\", \"organization\": \"Liquid\", \"license\": \"Unknown\", \"median\": 765.7561202251927, \"total_output_tokens\": 1933376.0, \"conso_all_conv\": 15.96704347946668, \"n_match\": 2097, \"total_output_tokens_right\": 1933376.0, \"mean_conso_per_match\": 0.007614231511429032, \"mean_conso_per_token\": 8.258633333333341e-06, \"name_right\": \"Liquid Foundation Model\", \"organization_right\": \"Liquid\", \"license_right\": \"Unknown\"}, {\"name\": \"Nemotron 70B Instruct\", \"model_name\": \"llama-3.1-nemotron-70b-instruct\", \"organization\": \"Nvidia\", \"license\": \"Llama 3.1 Community\", \"median\": 1163.287438813354, \"total_output_tokens\": 3957200.0, \"conso_all_conv\": 49.341271546666775, \"n_match\": 2346, \"total_output_tokens_right\": 3957200.0, \"mean_conso_per_match\": 0.021032085058255233, \"mean_conso_per_token\": 1.246873333333336e-05, \"name_right\": \"Nemotron 70B Instruct\", \"organization_right\": \"Nvidia\", \"license_right\": \"Llama 3.1 Community\"}, {\"name\": \"Mistral Nemo Instruct\", \"model_name\": \"mistral-nemo-2407\", \"organization\": \"Mistral AI\", \"license\": \"Apache 2.0\", \"median\": 804.604573329981, \"total_output_tokens\": 2101470.0, \"conso_all_conv\": 9.09769793379999, \"n_match\": 3225, \"total_output_tokens_right\": 2101470.0, \"mean_conso_per_match\": 0.002820991607379842, \"mean_conso_per_token\": 4.329206666666662e-06, \"name_right\": \"Mistral Nemo Instruct\", \"organization_right\": \"Mistral AI\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Mistral Small-3.1 24B\", \"model_name\": \"mistral-small-3.1-24b\", \"organization\": \"Mistral AI\", \"license\": \"Apache 2.0\", \"median\": 1031.3332867794068, \"total_output_tokens\": 841162.0, \"conso_all_conv\": 5.058114592626664, \"n_match\": 824, \"total_output_tokens_right\": 841162.0, \"mean_conso_per_match\": 0.006138488583284786, \"mean_conso_per_token\": 6.013246666666663e-06, \"name_right\": \"Mistral Small-3.1 24B\", \"organization_right\": \"Mistral AI\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Mixtral 8x22B Instruct\", \"model_name\": \"mixtral-8x22b-instruct-v0.1\", \"organization\": \"Mistral AI\", \"license\": \"Apache 2.0\", \"median\": 846.785454718372, \"total_output_tokens\": 2889185.0, \"conso_all_conv\": 50.96510783260014, \"n_match\": 3160, \"total_output_tokens_right\": 2889185.0, \"mean_conso_per_match\": 0.016128198681202577, \"mean_conso_per_token\": 1.763996000000005e-05, \"name_right\": \"Mixtral 8x22B Instruct\", \"organization_right\": \"Mistral AI\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Mixtral 8x7B Instruct\", \"model_name\": \"mixtral-8x7b-instruct-v0.1\", \"organization\": \"Mistral AI\", \"license\": \"Apache 2.0\", \"median\": 814.8087648946796, \"total_output_tokens\": 900068.0, \"conso_all_conv\": 4.14920547184, \"n_match\": 1151, \"total_output_tokens_right\": 900068.0, \"mean_conso_per_match\": 0.003604870088479583, \"mean_conso_per_token\": 4.60988e-06, \"name_right\": \"Mixtral 8x7B Instruct\", \"organization_right\": \"Mistral AI\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Phi-3.5 Mini Instruct\", \"model_name\": \"phi-3.5-mini-instruct\", \"organization\": \"Microsoft\", \"license\": \"MIT\", \"median\": 797.5700958468177, \"total_output_tokens\": 851005.0, \"conso_all_conv\": 2.609331674216668, \"n_match\": 786, \"total_output_tokens_right\": 851005.0, \"mean_conso_per_match\": 0.003319760399766753, \"mean_conso_per_token\": 3.066176666666668e-06, \"name_right\": \"Phi-3.5 Mini Instruct\", \"organization_right\": \"Microsoft\", \"license_right\": \"MIT\"}, {\"name\": \"Phi 4\", \"model_name\": \"phi-4\", \"organization\": \"Microsoft\", \"license\": \"MIT\", \"median\": 957.6144707146054, \"total_output_tokens\": 3086417.0, \"conso_all_conv\": 14.228011999959978, \"n_match\": 2927, \"total_output_tokens_right\": 3086417.0, \"mean_conso_per_match\": 0.004860953877676794, \"mean_conso_per_token\": 4.609879999999993e-06, \"name_right\": \"Phi 4\", \"organization_right\": \"Microsoft\", \"license_right\": \"MIT\"}, {\"name\": \"QwQ 32B\", \"model_name\": \"qwq-32b\", \"organization\": \"Alibaba\", \"license\": \"Apache 2.0\", \"median\": 915.7228106812046, \"total_output_tokens\": 946986.0, \"conso_all_conv\": 6.757635276840006, \"n_match\": 577, \"total_output_tokens_right\": 946986.0, \"mean_conso_per_match\": 0.011711672923466214, \"mean_conso_per_token\": 7.135940000000006e-06, \"name_right\": \"QwQ 32B\", \"organization_right\": \"Alibaba\", \"license_right\": \"Apache 2.0\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rank_comparia.plot import draw_frugality_chart\n",
    "\n",
    "final_df = final_df.rename({\"elo_score\": \"median\"})\n",
    "draw_frugality_chart(final_df, title=\"consommation selon classement\", log=True, scale=\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
