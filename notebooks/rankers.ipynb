{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul de scores à partir du jeu de données `comparia`\n",
    "\n",
    "Dans ce notebook nous illustrons l'utilisation des classes `Ranker` pour calculer des scores à partir des données `comparia`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "cache_dir = input(\"Indicate path to all Hugging Face caches:\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = cache_dir\n",
    "os.environ[\"HF_HUB_CACHE\"] = cache_dir\n",
    "os.environ[\"HF_TOKEN\"] = getpass(\"Enter your HuggingFace token:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since ministere-culture/comparia-reactions couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'default' at /home/jupyterhub-users/shared/projet_comparia/huggingface_hub/ministere-culture___comparia-reactions/default/0.0.0/92a324c10228176065909b52bbbaa16430e64c5a (last modified on Wed Jun  4 17:40:33 2025).\n",
      "Using the latest cached version of the dataset since ministere-culture/comparia-conversations couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'default' at /home/jupyterhub-users/shared/projet_comparia/huggingface_hub/ministere-culture___comparia-conversations/default/0.0.0/dc40af6af1c14e68bf39d55f6e1573d2d6582f19 (last modified on Wed Jun  4 17:40:30 2025).\n"
     ]
    }
   ],
   "source": [
    "from rank_comparia.utils import load_comparia\n",
    "\n",
    "reactions = load_comparia(\"ministere-culture/comparia-reactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en forme des données\n",
    "\n",
    "Ici nous utilisons des fonctions *legacy* avec une heuristique simple pour déterminer le résultat d'un *match* (une paire de conversation) à partir des réactions associées. On soustrait le nombre de réactions négatives au nombre de réactions positives pour chaque modèle. Le modèle avec la différence la plus élevée est vainqueur du match. Si les différences sont identiques pour les deux modèle, le *match* est une égalité (on filtre les égalités dans la fonction `get_winners`).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_a_name</th><th>model_b_name</th><th>conversation_pair_id</th><th>score_a</th><th>score_b</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;qwen2.5-coder-32b-instruct&quot;</td><td>&quot;mistral-small-24b-instruct-250…</td><td>&quot;bcda2b64d8274b1087921c0b7cc7d8…</td><td>1</td><td>0</td></tr><tr><td>&quot;hermes-3-llama-3.1-405b&quot;</td><td>&quot;command-a&quot;</td><td>&quot;ecefa182135e4acc9cf04be13b1145…</td><td>0</td><td>1</td></tr><tr><td>&quot;mixtral-8x22b-instruct-v0.1&quot;</td><td>&quot;gemma-2-9b-it&quot;</td><td>&quot;bae309920e444c9ea73dbc977b5584…</td><td>0</td><td>-1</td></tr><tr><td>&quot;phi-4&quot;</td><td>&quot;llama-3.1-70b&quot;</td><td>&quot;354dce524e094fd3a0d39bb0422154…</td><td>2</td><td>0</td></tr><tr><td>&quot;mistral-large-2411&quot;</td><td>&quot;llama-3.1-nemotron-70b-instruc…</td><td>&quot;1739b0fd5fb34f43a182863a6f3f55…</td><td>0</td><td>2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌──────────────────────────┬─────────────────────────┬─────────────────────────┬─────────┬─────────┐\n",
       "│ model_a_name             ┆ model_b_name            ┆ conversation_pair_id    ┆ score_a ┆ score_b │\n",
       "│ ---                      ┆ ---                     ┆ ---                     ┆ ---     ┆ ---     │\n",
       "│ str                      ┆ str                     ┆ str                     ┆ i64     ┆ i64     │\n",
       "╞══════════════════════════╪═════════════════════════╪═════════════════════════╪═════════╪═════════╡\n",
       "│ qwen2.5-coder-32b-instru ┆ mistral-small-24b-instr ┆ bcda2b64d8274b1087921c0 ┆ 1       ┆ 0       │\n",
       "│ ct                       ┆ uct-250…                ┆ b7cc7d8…                ┆         ┆         │\n",
       "│ hermes-3-llama-3.1-405b  ┆ command-a               ┆ ecefa182135e4acc9cf04be ┆ 0       ┆ 1       │\n",
       "│                          ┆                         ┆ 13b1145…                ┆         ┆         │\n",
       "│ mixtral-8x22b-instruct-v ┆ gemma-2-9b-it           ┆ bae309920e444c9ea73dbc9 ┆ 0       ┆ -1      │\n",
       "│ 0.1                      ┆                         ┆ 77b5584…                ┆         ┆         │\n",
       "│ phi-4                    ┆ llama-3.1-70b           ┆ 354dce524e094fd3a0d39bb ┆ 2       ┆ 0       │\n",
       "│                          ┆                         ┆ 0422154…                ┆         ┆         │\n",
       "│ mistral-large-2411       ┆ llama-3.1-nemotron-70b- ┆ 1739b0fd5fb34f43a182863 ┆ 0       ┆ 2       │\n",
       "│                          ┆ instruc…                ┆ a6f3f55…                ┆         ┆         │\n",
       "└──────────────────────────┴─────────────────────────┴─────────────────────────┴─────────┴─────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rank_comparia.data_transformation import get_matches_with_score, get_winners, get_winrates\n",
    "\n",
    "matches = get_matches_with_score(reactions)\n",
    "\n",
    "matches.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = get_winners(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule des taux de victoire par modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (48, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_name</th><th>len</th><th>wins</th><th>winrate</th></tr><tr><td>str</td><td>u32</td><td>u32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;gemini-2.0-flash-exp&quot;</td><td>856</td><td>647</td><td>75.584112</td></tr><tr><td>&quot;deepseek-v3-chat&quot;</td><td>1530</td><td>1080</td><td>70.588235</td></tr><tr><td>&quot;gemma-3-27b&quot;</td><td>495</td><td>348</td><td>70.30303</td></tr><tr><td>&quot;gemini-2.0-flash-001&quot;</td><td>688</td><td>480</td><td>69.767442</td></tr><tr><td>&quot;gemini-1.5-pro-001&quot;</td><td>328</td><td>222</td><td>67.682927</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;mixtral-8x7b-instruct-v0.1&quot;</td><td>585</td><td>222</td><td>37.948718</td></tr><tr><td>&quot;lfm-40b&quot;</td><td>887</td><td>321</td><td>36.189402</td></tr><tr><td>&quot;mixtral-8x22b-instruct-v0.1&quot;</td><td>1459</td><td>445</td><td>30.500343</td></tr><tr><td>&quot;mistral-nemo-2407&quot;</td><td>1461</td><td>435</td><td>29.774127</td></tr><tr><td>&quot;chocolatine-2-14b-instruct-v2.…</td><td>499</td><td>127</td><td>25.450902</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (48, 4)\n",
       "┌─────────────────────────────────┬──────┬──────┬───────────┐\n",
       "│ model_name                      ┆ len  ┆ wins ┆ winrate   │\n",
       "│ ---                             ┆ ---  ┆ ---  ┆ ---       │\n",
       "│ str                             ┆ u32  ┆ u32  ┆ f64       │\n",
       "╞═════════════════════════════════╪══════╪══════╪═══════════╡\n",
       "│ gemini-2.0-flash-exp            ┆ 856  ┆ 647  ┆ 75.584112 │\n",
       "│ deepseek-v3-chat                ┆ 1530 ┆ 1080 ┆ 70.588235 │\n",
       "│ gemma-3-27b                     ┆ 495  ┆ 348  ┆ 70.30303  │\n",
       "│ gemini-2.0-flash-001            ┆ 688  ┆ 480  ┆ 69.767442 │\n",
       "│ gemini-1.5-pro-001              ┆ 328  ┆ 222  ┆ 67.682927 │\n",
       "│ …                               ┆ …    ┆ …    ┆ …         │\n",
       "│ mixtral-8x7b-instruct-v0.1      ┆ 585  ┆ 222  ┆ 37.948718 │\n",
       "│ lfm-40b                         ┆ 887  ┆ 321  ┆ 36.189402 │\n",
       "│ mixtral-8x22b-instruct-v0.1     ┆ 1459 ┆ 445  ┆ 30.500343 │\n",
       "│ mistral-nemo-2407               ┆ 1461 ┆ 435  ┆ 29.774127 │\n",
       "│ chocolatine-2-14b-instruct-v2.… ┆ 499  ┆ 127  ┆ 25.450902 │\n",
       "└─────────────────────────────────┴──────┴──────┴───────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winrates = get_winrates(winners)\n",
    "winrates.sort(\"winrate\", descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul des scores\n",
    "\n",
    "Pour chaque *match* on calcule un score, on mélange les matchs et on les ajoute un par un à un `ELORanker` qui met à jour les scores à chaque ajout de match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_comparia.elo import ELORanker\n",
    "from rank_comparia.ranker import Match, MatchScore\n",
    "import random\n",
    "\n",
    "\n",
    "def compute_match_score(score_a: int, score_b: int) -> MatchScore:\n",
    "    final_score = score_b - score_a\n",
    "    if final_score > 0:\n",
    "        return MatchScore.B\n",
    "    elif final_score < 0:\n",
    "        return MatchScore.A\n",
    "    else:\n",
    "        return MatchScore.Draw\n",
    "\n",
    "\n",
    "def get_shuffled_results(matches: list[Match], model_names: list[str], seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    ranker_shuffle = ELORanker(K=40)\n",
    "    matches_shuffle = random.sample(matches, k=len(matches))\n",
    "    ranker_shuffle.add_players(model_names)\n",
    "    ranker_shuffle.compute_scores(matches=matches_shuffle)\n",
    "    return ranker_shuffle.players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 sets de scores sont calculés avec des ordres d'ajout des matchs différents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = set(matches[\"model_a_name\"].unique()) | set(matches[\"model_b_name\"].unique())\n",
    "matches = [\n",
    "    Match(\n",
    "        match_dict[\"model_a_name\"],\n",
    "        match_dict[\"model_b_name\"],\n",
    "        compute_match_score(match_dict[\"score_a\"], match_dict[\"score_b\"]),\n",
    "    )\n",
    "    for match_dict in matches.to_dicts()\n",
    "]\n",
    "\n",
    "player_results = {\n",
    "    seed: get_shuffled_results(matches=matches, model_names=model_names, seed=seed) for seed in range(100)  # type: ignore\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les scores moyens sont calculés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_avg_ranking = {\n",
    "    player_name: sum(results[player_name] for results in player_results.values()) / 100 for player_name in model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash-exp : 1150.7875539940342\n",
      "gemma-3-27b : 1146.1814815271186\n",
      "gemini-2.0-flash-001 : 1145.8797934782958\n",
      "deepseek-v3-0324 : 1120.69196210039\n",
      "claude-3-7-sonnet : 1116.8451653200439\n",
      "deepseek-v3-chat : 1112.2859940634464\n",
      "command-a : 1099.1468132613213\n",
      "gpt-4.1-mini : 1093.3035931469067\n",
      "gemma-3-12b : 1079.6231164328428\n",
      "grok-3-mini-beta : 1075.3302854861156\n",
      "llama-3.1-nemotron-70b-instruct : 1075.2449628593781\n",
      "deepseek-r1 : 1069.4332452165186\n",
      "gemini-1.5-pro-002 : 1059.8031897395003\n",
      "gemma-3-4b : 1050.277091070327\n",
      "llama-4-scout : 1042.615116941869\n",
      "gemini-1.5-pro-001 : 1038.5672651384687\n",
      "mistral-large-2411 : 1027.209557652193\n",
      "mistral-small-3.1-24b : 1026.3386272941707\n",
      "o3-mini : 1013.0501472659827\n",
      "claude-3-5-sonnet-v2 : 1009.8067470339537\n",
      "o4-mini : 1003.4964558569891\n",
      "llama-3.3-70b : 1001.1886684951511\n",
      "gpt-4o-mini-2024-07-18 : 998.4060853891027\n",
      "gpt-4o-2024-08-06 : 994.4362943268783\n",
      "mistral-saba : 994.2686844396636\n",
      "llama-3.1-405b : 993.0428972867334\n",
      "mistral-small-24b-instruct-2501 : 988.040331408866\n",
      "gpt-4.1-nano : 985.6441334694255\n",
      "jamba-1.5-large : 981.5801866138955\n",
      "gemma-2-27b-it-q8 : 977.2315815741808\n",
      "llama-3.1-70b : 971.1846902139217\n",
      "gemma-2-9b-it : 970.6622549978354\n",
      "phi-4 : 969.8068392149897\n",
      "deepseek-r1-distill-llama-70b : 965.7473659809484\n",
      "aya-expanse-8b : 964.7894738168814\n",
      "qwq-32b : 961.1234056508993\n",
      "ministral-8b-instruct-2410 : 954.2805969367718\n",
      "hermes-3-llama-3.1-405b : 934.7697487787314\n",
      "c4ai-command-r-08-2024 : 929.9418491627882\n",
      "llama-3.1-8b : 919.6619575226501\n",
      "qwen2.5-coder-32b-instruct : 915.5060238460749\n",
      "qwen2.5-7b-instruct : 904.2429989849309\n",
      "phi-3.5-mini-instruct : 889.651782763259\n",
      "lfm-40b : 887.1828404850276\n",
      "mixtral-8x7b-instruct-v0.1 : 876.1274255158394\n",
      "mixtral-8x22b-instruct-v0.1 : 846.5497703378513\n",
      "mistral-nemo-2407 : 845.610883608734\n",
      "chocolatine-2-14b-instruct-v2.0.3-q8 : 823.4030642981037\n"
     ]
    }
   ],
   "source": [
    "for player, ranking in sorted(players_avg_ranking.items(), key=lambda x: -x[1]):\n",
    "    print(f\"{player} : {ranking}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deux calculs des scores avec des ordres de matchs différents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'command-a': 1220.1527290800668,\n",
       " 'gemini-2.0-flash-exp': 1161.6044499773263,\n",
       " 'deepseek-v3-chat': 1150.560015291643,\n",
       " 'gemma-3-27b': 1141.4328148299785,\n",
       " 'gemini-2.0-flash-001': 1139.5359177795244,\n",
       " 'deepseek-v3-0324': 1119.6334478686797,\n",
       " 'grok-3-mini-beta': 1119.5307837389546,\n",
       " 'llama-4-scout': 1091.5751639816478,\n",
       " 'llama-3.1-nemotron-70b-instruct': 1091.0795137063953,\n",
       " 'claude-3-7-sonnet': 1088.7675701005921,\n",
       " 'mistral-small-3.1-24b': 1070.6355146154658,\n",
       " 'gpt-4.1-mini': 1050.3170643862245,\n",
       " 'mistral-saba': 1041.3557054625712,\n",
       " 'gemini-1.5-pro-001': 1041.2486761693076,\n",
       " 'mistral-small-24b-instruct-2501': 1040.8327831881509,\n",
       " 'llama-3.3-70b': 1039.0387741753636,\n",
       " 'gemma-3-12b': 1031.760023662148,\n",
       " 'gemma-3-4b': 1023.6936864525285,\n",
       " 'deepseek-r1-distill-llama-70b': 1023.4496226956928,\n",
       " 'qwq-32b': 1020.8316195161804,\n",
       " 'gemini-1.5-pro-002': 1014.5163572166467,\n",
       " 'phi-4': 1013.0522119901723,\n",
       " 'o4-mini': 1003.6818359030656,\n",
       " 'o3-mini': 997.813962646233,\n",
       " 'gemma-2-9b-it': 993.9508230378204,\n",
       " 'gpt-4.1-nano': 991.5317530448987,\n",
       " 'gpt-4o-2024-08-06': 990.4189007296627,\n",
       " 'deepseek-r1': 989.2963540178825,\n",
       " 'claude-3-5-sonnet-v2': 984.5266967113974,\n",
       " 'hermes-3-llama-3.1-405b': 980.896805091614,\n",
       " 'gpt-4o-mini-2024-07-18': 975.6064843044875,\n",
       " 'gemma-2-27b-it-q8': 968.1510440546857,\n",
       " 'llama-3.1-405b': 965.5426286488934,\n",
       " 'ministral-8b-instruct-2410': 956.2311939239934,\n",
       " 'aya-expanse-8b': 954.3752759002571,\n",
       " 'mistral-large-2411': 954.1603591745562,\n",
       " 'c4ai-command-r-08-2024': 949.8540066306145,\n",
       " 'lfm-40b': 946.0294204238477,\n",
       " 'jamba-1.5-large': 929.4497120461755,\n",
       " 'llama-3.1-70b': 916.3639377496953,\n",
       " 'mixtral-8x22b-instruct-v0.1': 912.1088145284641,\n",
       " 'qwen2.5-coder-32b-instruct': 906.6726441859098,\n",
       " 'chocolatine-2-14b-instruct-v2.0.3-q8': 894.9892494495518,\n",
       " 'qwen2.5-7b-instruct': 883.146213312535,\n",
       " 'llama-3.1-8b': 869.8788851374325,\n",
       " 'mixtral-8x7b-instruct-v0.1': 849.1499476648862,\n",
       " 'phi-3.5-mini-instruct': 769.5320658565763,\n",
       " 'mistral-nemo-2407': 732.0365399396096}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker_shuffle = ELORanker(K=40)\n",
    "\n",
    "random.seed(42)\n",
    "matches_shuffle = random.sample(matches, k=len(matches))\n",
    "ranker_shuffle.add_players(model_names)  # type: ignore\n",
    "ranker_shuffle.compute_scores(matches=matches_shuffle)\n",
    "ranker_shuffle.get_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deepseek-v3-chat': 1179.3730081196136,\n",
       " 'gemma-3-27b': 1150.8317142104368,\n",
       " 'gemini-2.0-flash-001': 1143.7168434136654,\n",
       " 'grok-3-mini-beta': 1142.6570863406994,\n",
       " 'mistral-large-2411': 1142.1452498662647,\n",
       " 'llama-3.1-nemotron-70b-instruct': 1131.4665117329307,\n",
       " 'deepseek-r1': 1130.811884553102,\n",
       " 'gemma-3-12b': 1112.5530413171457,\n",
       " 'gemma-2-9b-it': 1096.048355031984,\n",
       " 'deepseek-v3-0324': 1094.993840729807,\n",
       " 'gemini-2.0-flash-exp': 1081.5379183680243,\n",
       " 'gemini-1.5-pro-002': 1075.4348182301687,\n",
       " 'command-a': 1072.7119341969287,\n",
       " 'llama-4-scout': 1067.852953995856,\n",
       " 'mistral-small-3.1-24b': 1063.5642262327028,\n",
       " 'claude-3-5-sonnet-v2': 1053.0858633064097,\n",
       " 'o3-mini': 1037.6401080954292,\n",
       " 'o4-mini': 1034.6809769907807,\n",
       " 'qwen2.5-coder-32b-instruct': 1031.3418629004047,\n",
       " 'gemma-3-4b': 1027.9349699340958,\n",
       " 'mistral-saba': 1015.2707776128149,\n",
       " 'mistral-small-24b-instruct-2501': 1014.3792660281887,\n",
       " 'aya-expanse-8b': 1013.7532922323962,\n",
       " 'llama-3.3-70b': 1010.9963606887425,\n",
       " 'claude-3-7-sonnet': 1004.8169484976083,\n",
       " 'gemma-2-27b-it-q8': 1003.7774799678639,\n",
       " 'ministral-8b-instruct-2410': 1002.215058759594,\n",
       " 'gpt-4.1-mini': 1001.4782695767303,\n",
       " 'jamba-1.5-large': 990.2443832619517,\n",
       " 'llama-3.1-405b': 987.6202667766365,\n",
       " 'gpt-4o-2024-08-06': 978.9458268001398,\n",
       " 'hermes-3-llama-3.1-405b': 974.685414001438,\n",
       " 'llama-3.1-8b': 969.8439024556758,\n",
       " 'gemini-1.5-pro-001': 966.3550344156399,\n",
       " 'c4ai-command-r-08-2024': 958.8780665535696,\n",
       " 'llama-3.1-70b': 949.4028454026375,\n",
       " 'gpt-4.1-nano': 931.9990319806918,\n",
       " 'qwq-32b': 921.547592841107,\n",
       " 'mixtral-8x7b-instruct-v0.1': 921.3460534819411,\n",
       " 'gpt-4o-mini-2024-07-18': 913.697314309419,\n",
       " 'deepseek-r1-distill-llama-70b': 892.966885249916,\n",
       " 'qwen2.5-7b-instruct': 879.8015732325656,\n",
       " 'phi-4': 831.0645231865586,\n",
       " 'lfm-40b': 823.2764445394187,\n",
       " 'mixtral-8x22b-instruct-v0.1': 807.2039198875214,\n",
       " 'chocolatine-2-14b-instruct-v2.0.3-q8': 800.9714878477747,\n",
       " 'mistral-nemo-2407': 800.2959353469424,\n",
       " 'phi-3.5-mini-instruct': 762.7828774980769}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker_shuffle = ELORanker(K=40)\n",
    "\n",
    "random.seed(1337)\n",
    "matches_shuffle = random.sample(matches, k=len(matches))\n",
    "ranker_shuffle.add_players(model_names)  # type: ignore\n",
    "ranker_shuffle.compute_scores(matches=matches_shuffle)\n",
    "ranker_shuffle.get_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on calcule les scores avec un `Ranker` alternatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gemini-2.0-flash-exp': np.float64(1143.1512358130012),\n",
       " 'gemma-3-27b': np.float64(1136.3184203836174),\n",
       " 'gemini-2.0-flash-001': np.float64(1133.400570032988),\n",
       " 'deepseek-v3-chat': np.float64(1114.0328789363393),\n",
       " 'deepseek-v3-0324': np.float64(1113.2067915235539),\n",
       " 'claude-3-7-sonnet': np.float64(1105.1612084450092),\n",
       " 'command-a': np.float64(1101.8786858020478),\n",
       " 'gpt-4.1-mini': np.float64(1093.6685242271394),\n",
       " 'gemma-3-12b': np.float64(1081.4653106195133),\n",
       " 'llama-3.1-nemotron-70b-instruct': np.float64(1078.196872612909),\n",
       " 'grok-3-mini-beta': np.float64(1071.2724072426324),\n",
       " 'deepseek-r1': np.float64(1064.2931624791943),\n",
       " 'gemma-3-4b': np.float64(1055.3168065207053),\n",
       " 'gemini-1.5-pro-002': np.float64(1047.884270532969),\n",
       " 'llama-4-scout': np.float64(1038.2019664020102),\n",
       " 'gemini-1.5-pro-001': np.float64(1037.4923002507746),\n",
       " 'mistral-small-3.1-24b': np.float64(1030.0445144875346),\n",
       " 'mistral-large-2411': np.float64(1028.6925455484807),\n",
       " 'o3-mini': np.float64(1006.2210001085826),\n",
       " 'claude-3-5-sonnet-v2': np.float64(1002.9586930415215),\n",
       " 'o4-mini': np.float64(1002.2210887330519),\n",
       " 'gpt-4o-mini-2024-07-18': np.float64(999.3491370088385),\n",
       " 'mistral-saba': np.float64(996.2399607255587),\n",
       " 'gpt-4o-2024-08-06': np.float64(993.5576631670547),\n",
       " 'llama-3.3-70b': np.float64(990.765165414004),\n",
       " 'llama-3.1-405b': np.float64(990.3338930578011),\n",
       " 'gpt-4.1-nano': np.float64(987.5804923433841),\n",
       " 'jamba-1.5-large': np.float64(985.7386697199976),\n",
       " 'mistral-small-24b-instruct-2501': np.float64(985.0343495551712),\n",
       " 'phi-4': np.float64(980.6013968645213),\n",
       " 'gemma-2-27b-it-q8': np.float64(974.6081627145344),\n",
       " 'llama-3.1-70b': np.float64(968.6630865442116),\n",
       " 'gemma-2-9b-it': np.float64(967.0569404993886),\n",
       " 'deepseek-r1-distill-llama-70b': np.float64(961.7384805979112),\n",
       " 'aya-expanse-8b': np.float64(960.6598616976838),\n",
       " 'qwq-32b': np.float64(958.2356609829194),\n",
       " 'ministral-8b-instruct-2410': np.float64(957.8237000706683),\n",
       " 'hermes-3-llama-3.1-405b': np.float64(939.7716775444147),\n",
       " 'c4ai-command-r-08-2024': np.float64(934.8510999183768),\n",
       " 'llama-3.1-8b': np.float64(928.9460566541117),\n",
       " 'qwen2.5-coder-32b-instruct': np.float64(927.4127141222015),\n",
       " 'qwen2.5-7b-instruct': np.float64(912.2611208222494),\n",
       " 'lfm-40b': np.float64(896.5231395095763),\n",
       " 'phi-3.5-mini-instruct': np.float64(889.3352897771736),\n",
       " 'mixtral-8x7b-instruct-v0.1': np.float64(883.6849028582083),\n",
       " 'mixtral-8x22b-instruct-v0.1': np.float64(859.7838306178774),\n",
       " 'mistral-nemo-2407': np.float64(855.2771876657266),\n",
       " 'chocolatine-2-14b-instruct-v2.0.3-q8': np.float64(829.08710580286)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rank_comparia.maximum_likelihood import MaximumLikelihoodRanker\n",
    "\n",
    "ranker = MaximumLikelihoodRanker()\n",
    "ranker.compute_scores(matches=matches)\n",
    "ranker.get_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les classes `Ranker` ont une méthode `compute_boostrap_scores` qui permettent de calculer des scores et intervalles de confiance bootstrap (les matchs qui servent au calcul des scores pour chaque échantillon bootstrap sont issus de ré-échantillonages avec remise de l'échantillon de matchs initial). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing bootstrap scores from a sample of 22993 matches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing bootstrap samples: 100%|██████████| 100/100 [00:05<00:00, 17.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (48, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>median</th><th>p2.5</th><th>p97.5</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;gemma-3-27b&quot;</td><td>1152.211148</td><td>1040.594581</td><td>1248.836173</td></tr><tr><td>&quot;gemini-2.0-flash-exp&quot;</td><td>1145.397963</td><td>1062.294185</td><td>1245.563045</td></tr><tr><td>&quot;deepseek-v3-0324&quot;</td><td>1134.799235</td><td>1013.055123</td><td>1228.182057</td></tr><tr><td>&quot;gemini-2.0-flash-001&quot;</td><td>1133.382779</td><td>1038.849998</td><td>1255.32273</td></tr><tr><td>&quot;deepseek-v3-chat&quot;</td><td>1114.914604</td><td>1013.009749</td><td>1228.08963</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;phi-3.5-mini-instruct&quot;</td><td>885.697997</td><td>761.038555</td><td>989.372198</td></tr><tr><td>&quot;mixtral-8x7b-instruct-v0.1&quot;</td><td>866.404306</td><td>755.314356</td><td>1002.638954</td></tr><tr><td>&quot;mixtral-8x22b-instruct-v0.1&quot;</td><td>853.719716</td><td>758.372644</td><td>956.329336</td></tr><tr><td>&quot;mistral-nemo-2407&quot;</td><td>832.302051</td><td>743.147907</td><td>930.64927</td></tr><tr><td>&quot;chocolatine-2-14b-instruct-v2.…</td><td>816.066302</td><td>727.559257</td><td>928.074185</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (48, 4)\n",
       "┌─────────────────────────────────┬─────────────┬─────────────┬─────────────┐\n",
       "│ model                           ┆ median      ┆ p2.5        ┆ p97.5       │\n",
       "│ ---                             ┆ ---         ┆ ---         ┆ ---         │\n",
       "│ str                             ┆ f64         ┆ f64         ┆ f64         │\n",
       "╞═════════════════════════════════╪═════════════╪═════════════╪═════════════╡\n",
       "│ gemma-3-27b                     ┆ 1152.211148 ┆ 1040.594581 ┆ 1248.836173 │\n",
       "│ gemini-2.0-flash-exp            ┆ 1145.397963 ┆ 1062.294185 ┆ 1245.563045 │\n",
       "│ deepseek-v3-0324                ┆ 1134.799235 ┆ 1013.055123 ┆ 1228.182057 │\n",
       "│ gemini-2.0-flash-001            ┆ 1133.382779 ┆ 1038.849998 ┆ 1255.32273  │\n",
       "│ deepseek-v3-chat                ┆ 1114.914604 ┆ 1013.009749 ┆ 1228.08963  │\n",
       "│ …                               ┆ …           ┆ …           ┆ …           │\n",
       "│ phi-3.5-mini-instruct           ┆ 885.697997  ┆ 761.038555  ┆ 989.372198  │\n",
       "│ mixtral-8x7b-instruct-v0.1      ┆ 866.404306  ┆ 755.314356  ┆ 1002.638954 │\n",
       "│ mixtral-8x22b-instruct-v0.1     ┆ 853.719716  ┆ 758.372644  ┆ 956.329336  │\n",
       "│ mistral-nemo-2407               ┆ 832.302051  ┆ 743.147907  ┆ 930.64927   │\n",
       "│ chocolatine-2-14b-instruct-v2.… ┆ 816.066302  ┆ 727.559257  ┆ 928.074185  │\n",
       "└─────────────────────────────────┴─────────────┴─────────────┴─────────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker = ELORanker(K=40)\n",
    "\n",
    "ranker.add_players(model_names)  # type: ignore\n",
    "scores = ranker.compute_bootstrap_scores(matches=matches)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing bootstrap scores from a sample of 22993 matches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing bootstrap samples: 100%|██████████| 100/100 [00:12<00:00,  7.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (48, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>median</th><th>p2.5</th><th>p97.5</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;gemini-2.0-flash-exp&quot;</td><td>1143.325771</td><td>1125.07339</td><td>1169.492016</td></tr><tr><td>&quot;gemma-3-27b&quot;</td><td>1136.857481</td><td>1103.11539</td><td>1167.911554</td></tr><tr><td>&quot;gemini-2.0-flash-001&quot;</td><td>1134.308685</td><td>1109.516369</td><td>1153.092221</td></tr><tr><td>&quot;deepseek-v3-chat&quot;</td><td>1114.655428</td><td>1099.492036</td><td>1125.729753</td></tr><tr><td>&quot;deepseek-v3-0324&quot;</td><td>1111.016549</td><td>1071.932718</td><td>1150.501659</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;phi-3.5-mini-instruct&quot;</td><td>890.765449</td><td>859.732501</td><td>920.225684</td></tr><tr><td>&quot;mixtral-8x7b-instruct-v0.1&quot;</td><td>884.938064</td><td>865.35744</td><td>909.701876</td></tr><tr><td>&quot;mixtral-8x22b-instruct-v0.1&quot;</td><td>860.519985</td><td>845.704374</td><td>873.367552</td></tr><tr><td>&quot;mistral-nemo-2407&quot;</td><td>854.598367</td><td>836.824396</td><td>871.170898</td></tr><tr><td>&quot;chocolatine-2-14b-instruct-v2.…</td><td>829.642353</td><td>798.290301</td><td>853.690672</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (48, 4)\n",
       "┌─────────────────────────────────┬─────────────┬─────────────┬─────────────┐\n",
       "│ model                           ┆ median      ┆ p2.5        ┆ p97.5       │\n",
       "│ ---                             ┆ ---         ┆ ---         ┆ ---         │\n",
       "│ str                             ┆ f64         ┆ f64         ┆ f64         │\n",
       "╞═════════════════════════════════╪═════════════╪═════════════╪═════════════╡\n",
       "│ gemini-2.0-flash-exp            ┆ 1143.325771 ┆ 1125.07339  ┆ 1169.492016 │\n",
       "│ gemma-3-27b                     ┆ 1136.857481 ┆ 1103.11539  ┆ 1167.911554 │\n",
       "│ gemini-2.0-flash-001            ┆ 1134.308685 ┆ 1109.516369 ┆ 1153.092221 │\n",
       "│ deepseek-v3-chat                ┆ 1114.655428 ┆ 1099.492036 ┆ 1125.729753 │\n",
       "│ deepseek-v3-0324                ┆ 1111.016549 ┆ 1071.932718 ┆ 1150.501659 │\n",
       "│ …                               ┆ …           ┆ …           ┆ …           │\n",
       "│ phi-3.5-mini-instruct           ┆ 890.765449  ┆ 859.732501  ┆ 920.225684  │\n",
       "│ mixtral-8x7b-instruct-v0.1      ┆ 884.938064  ┆ 865.35744   ┆ 909.701876  │\n",
       "│ mixtral-8x22b-instruct-v0.1     ┆ 860.519985  ┆ 845.704374  ┆ 873.367552  │\n",
       "│ mistral-nemo-2407               ┆ 854.598367  ┆ 836.824396  ┆ 871.170898  │\n",
       "│ chocolatine-2-14b-instruct-v2.… ┆ 829.642353  ┆ 798.290301  ┆ 853.690672  │\n",
       "└─────────────────────────────────┴─────────────┴─────────────┴─────────────┘"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker = MaximumLikelihoodRanker()\n",
    "scores = ranker.compute_bootstrap_scores(matches=matches)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajout de la notion de frugalité dans le score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_comparia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrank_comparia\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfrugality\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_ranked_frugality, get_normalized_log_cost, calculate_frugality_score\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m conversations = \u001b[43mload_comparia\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mministere-culture/comparia-conversations\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m frugality_score = calculate_frugality_score(conversations, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m graph = draw_ranked_frugality(frugal_log_score=get_normalized_log_cost(frugality_score), bootstraped_scores=scores)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_comparia' is not defined"
     ]
    }
   ],
   "source": [
    "from rank_comparia.frugality import draw_ranked_frugality, get_normalized_log_cost, calculate_frugality_score\n",
    "\n",
    "conversations = load_comparia(\"ministere-culture/comparia-conversations\")\n",
    "frugality_score = calculate_frugality_score(conversations, None, True)\n",
    "graph = draw_ranked_frugality(frugal_log_score=get_normalized_log_cost(frugality_score), bootstraped_scores=scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgraph\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rank-comparia-mqEqqnXm-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
